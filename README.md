# **WAKEE : Work Assistant with Kindness & Emotional Empathy ðŸ§ ðŸ¤—**

### ðŸš€ Introduction
Bienvenue sur WAKEE (Work Assistant with Kindness & Emotional Empathy), votre assistant de travail intelligent conÃ§u pour amÃ©liorer votre concentration et votre bien-Ãªtre. Ce projet utilise la reconnaissance des Ã©motions par un modÃ¨le de rÃ©seau de neurones convolutionnel (CNN : EfficientNetB4 fine tunÃ©) combinÃ©e aux recommandations personnalisÃ©es d'un grand modÃ¨le linguistique (LLM : Mistral Small Latest) pour vous offrir un environnement de travail adaptÃ©, particuliÃ¨rement utile pour les personnes atteintes de TDAH.

WAKEE vous aide Ã  rester engagÃ© et productif en dÃ©tectant vos Ã©motions (ennui, engagement, confusion, frustration) et en vous proposant des suggestions proactives pour vous aider Ã  surmonter les dÃ©fis et Ã  maintenir votre attention.

### âœ¨ FonctionnalitÃ©s
- Reconnaissance d'Ã‰motions en Temps RÃ©el : Utilisation de votre camÃ©ra pour dÃ©tecter des Ã©motions clÃ©s telles que l'ennui, l'engagement, la confusion et la frustration grÃ¢ce Ã  un rÃ©seau de neurones convolutif (CNN).

- Recommandations PersonnalisÃ©es par LLM : Un grand modÃ¨le linguistique analyse les Ã©motions dÃ©tectÃ©es et gÃ©nÃ¨re des conseils et des stratÃ©gies adaptÃ©s pour vous aider Ã  retrouver votre concentration ou Ã  gÃ©rer votre Ã©tat Ã©motionnel.

- Gestion du Temps de Travail : Choisissez une durÃ©e de session de travail personnalisÃ©e et suivez votre progression grÃ¢ce Ã  une barre de temps visuelle.

- Interface Intuitive Streamlit : Une interface utilisateur claire et interactive, facilitant l'activation de la camÃ©ra, la visualisation des analyses en temps rÃ©el et la rÃ©ception des suggestions.

ConÃ§u pour le Bien-Ãªtre : Une approche axÃ©e sur l'empathie pour crÃ©er un environnement de travail plus serein et productif.

ðŸ› ï¸ Installation
Pour faire fonctionner WAKEE sur votre machine locale, suivez ces Ã©tapes :

- Cloner le dÃ©pÃ´t :
```bash
git clone https://github.com/JeremyM174/jedhaproject_tdahdetection
cd jedhaproject_tdahdetection
```

- CrÃ©er un environnement virtuel (recommandÃ©) :
```bash
python -m venv venv
# Sur Windows
.\venv\Scripts\activate
# Sur macOS/Linux
source venv/bin/activate
```

- Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

- Configurer sa ClÃ© API (LLM) :
CrÃ©er un fichier .env Ã  la racine du projet avec votre clÃ© API.
Exemple de fichier .env :

```bash
MISTRAL_API_KEY="votre_cle_api_mistral"
```
Vous pouvez vous mÃªme crÃ©er votre clÃ© API Mistral : [https://admin.mistral.ai/organization/api-keys]


### ðŸš€ Utilisation
Pour dÃ©marrer l'application Streamlit :

```Bash
streamlit run app.py
```

L'application s'ouvrira automatiquement dans votre navigateur par dÃ©faut.

### ðŸ’¡ Comment Ã§a Marche ?
- Choisissez votre temps de travail : Utilisez le curseur pour dÃ©finir la durÃ©e de votre session.

- Activez votre camÃ©ra : Cliquez sur le bouton "Activer ma camÃ©ra".

- Suivez votre progression : Une barre de progression vous montrera le temps Ã©coulÃ©.

- Recevez des suggestions de Wakee : L'assistant analysera vos expressions et vous proposera des recommandations contextuelles pour vous aider Ã  rester concentrÃ© et positif.

### Datasets

* **DAiSEE (Dataset for Affective States in E-Learning Environments)**
    * **Auteurs :**
        * Deepak S. Rajpoot
        * Tania Das
        * N. G. Gadge
        * Mayank Nag
        * Abhishek Kumar
        * Alok Kumar
    * **Source / Lien :** [https://daisee.org/](https://daisee.org/)
    * **Description :** Le dataset DAiSEE contient des vidÃ©os d'Ã©tudiants en train de suivre des tutoriels en ligne, annotÃ©es pour quatre Ã©tats affectifs (Ennui, Engagement, Frustration, Confusion) ainsi que le niveau d'engagement. Il a Ã©tÃ© utilisÃ© dans ce projet pour [expliquer briÃ¨vement comment vous l'avez utilisÃ©, ex: "l'entraÃ®nement d'un modÃ¨le de reconnaissance de l'engagement Ã©motionnel"]. Nous avons rÃ©cupÃ©rÃ© une version plus lÃ©gÃ¨re du dataset sous format photos sur Kaggle [https://www.kaggle.com/datasets/johnykletka12348/daiseecvproject]
    * **RÃ©fÃ©rence acadÃ©mique (si vous voulez Ãªtre trÃ¨s prÃ©cis) :** Si vous avez trouvÃ© une publication scientifique associÃ©e au dataset (souvent disponible sur le site du dataset ou via Google Scholar), vous pouvez ajouter une citation dans un format acadÃ©mique commun (par exemple, APA, IEEE, etc.), ou simplement le titre et la confÃ©rence/journal. Par exemple :
        > Rajpoot, D. S., Das, T., Gadge, N. G., Nag, M., Kumar, A., & Kumar, A. (2018). *DAiSEE: Towards VAE-LSTM based Emotion Recognition in E-Learning Environments*. (Ã€ complÃ©ter avec la confÃ©rence/journal exacte si vous la trouvez, ex: "Proceedings of the IEEE International Conference on Automatic Face & Gesture Recognition (FG)").

### ðŸ“ž Contact
Albert ROMANO - [https://github.com/Ter0rra]
Asma RHALMI - [https://github.com/Cauliflaa]
Jeremy MARIARGE - [https://github.com/JeremyM174]
Manon FAEDY - [https://github.com/ManonFAEDY]