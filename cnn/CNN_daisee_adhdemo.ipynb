{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMTE-gy8QrZx"
      },
      "source": [
        "**Étapes pour utiliser le code :**\n",
        "\n",
        "1. Préparez votre dataset comme décrit, avec les images dans un dossier et le fichier CSV des labels.\n",
        "\n",
        "2. Mettez à jour les variables DATA_DIR, CSV_FILE, IMAGE_DIR, et surtout la liste EMOTION_LABELS pour qu'elle corresponde exactement à vos émotions et l'ordre de vos colonnes dans le CSV.\n",
        "\n",
        "3. Exécutez le script.\n",
        "\n",
        "4. Surveillez la console pour les métriques d'entraînement et de validation. Ajustez les hyperparamètres (BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, FREEZE_FEATURES) si nécessaire en fonction des performances observées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLKa9aKKRG7N"
      },
      "source": [
        "**Organisation des données :**\n",
        "\n",
        "your_dataset/\n",
        "\n",
        "├── images/\n",
        "\n",
        "│   ├── image_001.jpg\n",
        "\n",
        "│   ├── image_002.jpg\n",
        "\n",
        "│   └── ...\n",
        "\n",
        "└── labels.csv # Ou un autre format de fichier, contenant les chemins d'images et leurs étiquettes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD1t0ktwWJ0U"
      },
      "source": [
        "**Format du dataset :**\n",
        "\n",
        "![Capture d'écran 2025-07-21 161638.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17t4rr1xS1ja"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models#, transforms\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import onnx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error \n",
        "import numpy as np\n",
        "from tqdm import tqdm # Pour les barres de progression\n",
        "\n",
        "#import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_vOu_5EUt30"
      },
      "source": [
        "On a le choix de fine tuner la dernière couche OU tout le modele (pas le même temps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# I/ Configuration et Hyperparamètres "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b1KN8LGLS6gP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilisation du périphérique : cuda:0\n"
          ]
        }
      ],
      "source": [
        "# --- 0. Configuration et Hyperparamètres ---\n",
        "DATA_DIR = r'C:\\Users\\alber\\Desktop\\visual_studio_code\\dossier_jedha\\Jedha_Full_stack\\00_Final_Project\\set\\DAiSEE_small' # Chemin vers votre dossier racine du dataset\n",
        "CSV_FILE = os.path.join(DATA_DIR, 'labels_daisee_continous.csv') # Chemin vers votre fichier CSV d'étiquettes\n",
        "IMAGE_DIR = os.path.join(DATA_DIR, 'Dataset') # Chemin vers le dossier contenant les images\n",
        "\n",
        "# Définissez vos émotions binaires ici, dans le même ordre que vos colonnes dans le CSV\n",
        "EMOTION_LABELS = ['boredom', \n",
        "                  'confusion',\n",
        "                  'engagement', \n",
        "                  'frustration'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(EMOTION_LABELS)\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "FREEZE_FEATURES = True # True pour ne fine-tuner que la dernière couche, False pour fine-tuner tout le modèle\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation du périphérique : {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# II/ Création du Dataset PyTorch personnalisé (modifié)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7i1HiG6PTReS"
      },
      "outputs": [],
      "source": [
        "# --- 1. Création du Dataset PyTorch personnalisé (modifié) ---\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, emotion_cols, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.emotion_cols = emotion_cols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image_path'])\n",
        "\n",
        "        emotion_labels = row[self.emotion_cols].values.astype(float)\n",
        "        emotion_labels = torch.tensor(emotion_labels, dtype=torch.float32)\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, emotion_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# III/ Transformations d'images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LkV1CWUbTUJB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\anaconda3\\envs\\dl_project_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Transformations d'images ---\n",
        "# Transformations pour l'entraînement (augmentation des données)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224), # Recadrage aléatoire et redimensionnement à 224x224\n",
        "        transforms.RandomHorizontalFlip(), # Retournement horizontal aléatoire\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Jitter de couleur\n",
        "        transforms.ToTensor(), # Convertir en Tensor PyTorch (met les pixels entre 0 et 1)\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalisation (moyenne et std ImageNet)\n",
        "    ]),\n",
        "    # Transformations pour la validation/test (juste redimensionnement et normalisation)\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256), # Redimensionner le plus petit côté à 256\n",
        "        transforms.CenterCrop(224), # Centre Crop à 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IV/ Chargement des données et division Train/Val "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-mNFgbICTrGD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement des données...\n",
            "Taille du dataset d'entraînement : 6335\n",
            "Taille du dataset de validation : 1584\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Chargement des données et division Train/Val ---\n",
        "print(\"Chargement des données...\")\n",
        "full_df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# Division du dataset en ensembles d'entraînement et de validation\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42) # Stratify si possible\n",
        "\n",
        "train_dataset = EmotionDataset(df=train_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, transform=data_transforms['train'])\n",
        "val_dataset = EmotionDataset(df=val_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "print(f\"Taille du dataset d'entraînement : {dataset_sizes['train']}\")\n",
        "print(f\"Taille du dataset de validation : {dataset_sizes['val']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V/ Chargement du modèle ResNet18 pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement et modification du modèle ResNet18 pré-entraîné...\n",
            "Freeze des couches convolutionnelles...\n",
            "features.7.1.block.0.1.weight\n",
            "features.7.1.block.0.1.bias\n",
            "features.7.1.block.1.0.weight\n",
            "features.7.1.block.1.1.weight\n",
            "features.7.1.block.1.1.bias\n",
            "features.7.1.block.2.fc1.weight\n",
            "features.7.1.block.2.fc1.bias\n",
            "features.7.1.block.2.fc2.weight\n",
            "features.7.1.block.2.fc2.bias\n",
            "features.7.1.block.3.0.weight\n",
            "features.7.1.block.3.1.weight\n",
            "features.7.1.block.3.1.bias\n",
            "features.8.0.weight\n",
            "features.8.1.weight\n",
            "features.8.1.bias\n",
            "classifier.1.weight\n",
            "classifier.1.bias\n",
            "Définition de la dernière couche...\n",
            "Modèle prêt ! ✅\n"
          ]
        }
      ],
      "source": [
        "print(\"Chargement et modification du modèle ResNet18 pré-entraîné...\")\n",
        "\n",
        "# 1. Chargement du modèle ResNet18 pré-entraîné sur ImageNet\n",
        "\n",
        "# choix 1 => Resnet18\n",
        "# model_ft = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# choix 2 => efficentnet\n",
        "model_ft = models.efficientnet_b4(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# # choix 2.1 => mobile net\n",
        "# model_ft = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
        "\n",
        "# choix 3 => efficentnet\n",
        "# model_ft = models.efficientnet_b7(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# choix 3  bis => convex_tiny \n",
        "\n",
        "# choix 4 => Resnet50\n",
        "# model_ft = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "\n",
        "print(\"Freeze des couches convolutionnelles...\")\n",
        "\n",
        "# 2. On gèle les couches de feature extraction\n",
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#fine_tune_at = len(list(model_ft.children())) - 10\n",
        "for name, param in list(model_ft.named_parameters())[-17:]:\n",
        "    print(name)\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"Définition de la dernière couche...\")\n",
        "\n",
        "# 3. Remplacement de la couche fully-connected finale\n",
        "\n",
        "# Resnet18\n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "# model_ft.fc = nn.Sequential(\n",
        "#     nn.Linear(num_ftrs, 512),\n",
        "#     nn.BatchNorm1d(512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.1),\n",
        "#     nn.Linear(512, NUM_CLASSES)\n",
        "# )\n",
        "\n",
        "# Efficientnet \n",
        "num_ftrs = model_ft.classifier[1].in_features\n",
        "\n",
        "model_ft.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(512, NUM_CLASSES)\n",
        ")\n",
        "\n",
        "# 4. Envoi du modèle sur le bon device (GPU ou CPU)\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "\n",
        "print(\"Modèle prêt ! ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "EfficientNet                                            [1, 4]                    --\n",
              "├─Sequential: 1-1                                       [1, 1792, 7, 7]           --\n",
              "│    └─Conv2dNormActivation: 2-1                        [1, 48, 112, 112]         --\n",
              "│    │    └─Conv2d: 3-1                                 [1, 48, 112, 112]         (1,296)\n",
              "│    │    └─BatchNorm2d: 3-2                            [1, 48, 112, 112]         (96)\n",
              "│    │    └─SiLU: 3-3                                   [1, 48, 112, 112]         --\n",
              "│    └─Sequential: 2-2                                  [1, 24, 112, 112]         --\n",
              "│    │    └─MBConv: 3-4                                 [1, 24, 112, 112]         (2,940)\n",
              "│    │    └─MBConv: 3-5                                 [1, 24, 112, 112]         (1,206)\n",
              "│    └─Sequential: 2-3                                  [1, 32, 56, 56]           --\n",
              "│    │    └─MBConv: 3-6                                 [1, 32, 56, 56]           (11,878)\n",
              "│    │    └─MBConv: 3-7                                 [1, 32, 56, 56]           (18,120)\n",
              "│    │    └─MBConv: 3-8                                 [1, 32, 56, 56]           (18,120)\n",
              "│    │    └─MBConv: 3-9                                 [1, 32, 56, 56]           (18,120)\n",
              "│    └─Sequential: 2-4                                  [1, 56, 28, 28]           --\n",
              "│    │    └─MBConv: 3-10                                [1, 56, 28, 28]           (25,848)\n",
              "│    │    └─MBConv: 3-11                                [1, 56, 28, 28]           (57,246)\n",
              "│    │    └─MBConv: 3-12                                [1, 56, 28, 28]           (57,246)\n",
              "│    │    └─MBConv: 3-13                                [1, 56, 28, 28]           (57,246)\n",
              "│    └─Sequential: 2-5                                  [1, 112, 14, 14]          --\n",
              "│    │    └─MBConv: 3-14                                [1, 112, 14, 14]          (70,798)\n",
              "│    │    └─MBConv: 3-15                                [1, 112, 14, 14]          (197,820)\n",
              "│    │    └─MBConv: 3-16                                [1, 112, 14, 14]          (197,820)\n",
              "│    │    └─MBConv: 3-17                                [1, 112, 14, 14]          (197,820)\n",
              "│    │    └─MBConv: 3-18                                [1, 112, 14, 14]          (197,820)\n",
              "│    │    └─MBConv: 3-19                                [1, 112, 14, 14]          (197,820)\n",
              "│    └─Sequential: 2-6                                  [1, 160, 14, 14]          --\n",
              "│    │    └─MBConv: 3-20                                [1, 160, 14, 14]          (240,924)\n",
              "│    │    └─MBConv: 3-21                                [1, 160, 14, 14]          (413,160)\n",
              "│    │    └─MBConv: 3-22                                [1, 160, 14, 14]          (413,160)\n",
              "│    │    └─MBConv: 3-23                                [1, 160, 14, 14]          (413,160)\n",
              "│    │    └─MBConv: 3-24                                [1, 160, 14, 14]          (413,160)\n",
              "│    │    └─MBConv: 3-25                                [1, 160, 14, 14]          (413,160)\n",
              "│    └─Sequential: 2-7                                  [1, 272, 7, 7]            --\n",
              "│    │    └─MBConv: 3-26                                [1, 272, 7, 7]            (520,904)\n",
              "│    │    └─MBConv: 3-27                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    │    └─MBConv: 3-28                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    │    └─MBConv: 3-29                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    │    └─MBConv: 3-30                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    │    └─MBConv: 3-31                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    │    └─MBConv: 3-32                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    │    └─MBConv: 3-33                                [1, 272, 7, 7]            (1,159,332)\n",
              "│    └─Sequential: 2-8                                  [1, 448, 7, 7]            --\n",
              "│    │    └─MBConv: 3-34                                [1, 448, 7, 7]            (1,420,804)\n",
              "│    │    └─MBConv: 3-35                                [1, 448, 7, 7]            3,049,200\n",
              "│    └─Conv2dNormActivation: 2-9                        [1, 1792, 7, 7]           --\n",
              "│    │    └─Conv2d: 3-36                                [1, 1792, 7, 7]           802,816\n",
              "│    │    └─BatchNorm2d: 3-37                           [1, 1792, 7, 7]           3,584\n",
              "│    │    └─SiLU: 3-38                                  [1, 1792, 7, 7]           --\n",
              "├─AdaptiveAvgPool2d: 1-2                                [1, 1792, 1, 1]           --\n",
              "├─Sequential: 1-3                                       [1, 4]                    --\n",
              "│    └─Linear: 2-10                                     [1, 512]                  918,016\n",
              "│    └─BatchNorm1d: 2-11                                [1, 512]                  1,024\n",
              "│    └─ReLU: 2-12                                       [1, 512]                  --\n",
              "│    └─Dropout: 2-13                                    [1, 512]                  --\n",
              "│    └─Linear: 2-14                                     [1, 4]                    2,052\n",
              "=========================================================================================================\n",
              "Total params: 18,469,708\n",
              "Trainable params: 3,572,468\n",
              "Non-trainable params: 14,897,240\n",
              "Total mult-adds (Units.GIGABYTES): 1.50\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 272.50\n",
              "Params size (MB): 73.88\n",
              "Estimated Total Size (MB): 346.98\n",
              "========================================================================================================="
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model_ft, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VI/ Fonction de perte et Optimiseur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def weighted_mse_loss(predictions, labels, weight_vector):\n",
        "#     # get squared error on predictions\n",
        "#     squared_diff = (predictions - labels)**2\n",
        "\n",
        "#     # find which labels are positive or not (this will define which losses we weight)\n",
        "#     positive_label_mask = (labels > 0)\n",
        "    \n",
        "#     # set weight to 1 if mask == 0 and weight otherwise\n",
        "#     weights = torch.where(positive_label_mask == 1, weight_vector, 1)\n",
        "\n",
        "#     weighted_squared_diff = weights * squared_diff\n",
        "\n",
        "#     return torch.mean(weighted_squared_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PAV_C7MYUH2A"
      },
      "outputs": [],
      "source": [
        "# --- 5. Fonction de perte et Optimiseur ---\n",
        "# Pour la classification multi-label, Binary Cross-Entropy with Logits Loss est la norme.\n",
        "# Elle combine une couche Sigmoid et la Binary Cross-Entropy.\n",
        "# Perte pour la classification multi-label des émotions\n",
        "\n",
        "# criterion_emotions = nn.BCEWithLogitsLoss(pos_weight=div2_emotion_weights)\n",
        "criterion_emotions = nn.MSELoss()\n",
        "# criterion_emotions = raph_fonction => in test\n",
        "\n",
        "# Seulement les paramètres qui nécessitent des gradients seront optimisés\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Ordonnanceur de taux d'apprentissage (réduit le LR après un certain nombre d'époques)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VII/ Fonction d'entraînement et d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Fonction d'entraînement et d'évaluation ---\n",
        "\n",
        "def train_model(model, \n",
        "                criterion_emotions, \n",
        "                optimizer, \n",
        "                train_loader, \n",
        "                val_loader, \n",
        "                device, \n",
        "                scheduler=None,\n",
        "                epochs=NUM_EPOCHS,\n",
        "                output_names=EMOTION_LABELS):\n",
        "    \n",
        "    since = time.time()\n",
        "    \n",
        "    history = {'train_loss': [],\n",
        "                'val_loss': [],\n",
        "                'val_MAE': [],\n",
        "                'val_MSE': [],\n",
        "                'val_RMSE': [],\n",
        "                'val_R2': []}\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print('-' * 30)\n",
        "\n",
        "        ### -------- TRAIN --------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion_emotions(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        ### -------- VAL --------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_predictions = []\n",
        "        val_true_values = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_emotions(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Convertir les tenseurs en numpy pour calculer les métriques sklearn\n",
        "                val_predictions.extend(outputs.cpu().numpy())\n",
        "                val_true_values.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "        # Convertir les listes en numpy array pour sklearn\n",
        "        val_true_values_np = np.array(val_true_values)\n",
        "        val_predictions_np = np.array(val_predictions)\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "        val_epoch_MAE = mean_absolute_error(val_true_values_np, val_predictions_np)\n",
        "        val_epoch_MSE = mean_squared_error(val_true_values_np, val_predictions_np)\n",
        "        val_epoch_RMSE = np.sqrt(mean_squared_error(val_true_values_np, val_predictions_np))\n",
        "        val_epoch_r2 = r2_score(val_true_values_np, val_predictions_np)\n",
        "        \n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "        history['val_MAE'].append(val_epoch_MAE)\n",
        "        history['val_MSE'].append(val_epoch_MSE)\n",
        "        history['val_RMSE'].append(val_epoch_RMSE)\n",
        "        history['val_R2'].append(val_epoch_r2)\n",
        "       \n",
        "        print(f\"Train Loss: {epoch_loss:.4f} | \"\n",
        "            f\"Val Loss: {val_epoch_loss:.4f} | \"\n",
        "            f\"Val MAE: {val_epoch_MAE:.4f} | \"\n",
        "            f\"Val MSE: {val_epoch_MSE:.4f} | \"\n",
        "            f\"Val RMSE: {val_epoch_RMSE:.4f} | \"\n",
        "            f\"Val R2: {val_epoch_r2:.4f} \\n \")\n",
        "        \n",
        "        all_metrics={}\n",
        "        \n",
        "        for i, name in enumerate(output_names):\n",
        "                true_i = val_true_values_np[:,i]\n",
        "                pred_i = val_predictions_np[:,i]\n",
        "                \n",
        "                mae_i = mean_absolute_error(true_i, pred_i)\n",
        "                mse_i = mean_squared_error(true_i, pred_i)\n",
        "                rmse_i = np.sqrt(mse_i)\n",
        "                r2_i = r2_score(true_i, pred_i)\n",
        "                \n",
        "                all_metrics[f'{name}_MAE'] = mae_i\n",
        "                all_metrics[f'{name}_MSE'] = mse_i\n",
        "                all_metrics[f'{name}_RMSE'] = rmse_i\n",
        "                all_metrics[f'{name}_R2'] = r2_i\n",
        "\n",
        "                print(f\"{name}: MAE: {mae_i:.4f}, MSE: {mse_i:.4f}, RMSE: {rmse_i:.4f}, R2 Score: {r2_i:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"\\n🕒 Entraînement terminé en {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2425740520.py, line 12)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m=> preprocess donne entre 0/1\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# # for i, name in enumerate(EMOTION_LABELS):\n",
        "# #     print(i)\n",
        "\n",
        "# option 1 \n",
        "# pour l'evaluation\n",
        "\n",
        "# => sortir mae mse \n",
        "\n",
        "# => recuperer les outputs et labels \n",
        "# regression => param de seuil => classification \n",
        "# classification report \n",
        "\n",
        "# option 2\n",
        "# => preprocess donne entre 0/1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VIII/ Lancement de l'entraînement "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Début de l'entraînement...\n",
            "\n",
            "\n",
            "Epoch 1/2\n",
            "------------------------------\n",
            "Train Loss: 0.7055 | Val Loss: 0.4864 | Val MAE: 0.5461 | Val MSE: 0.4864 | Val RMSE: 0.6974 | Val R2: 0.0174 \n",
            " \n",
            "boredom: MAE: 0.7117, MSE: 0.7205, RMSE: 0.8488, R2 Score: 0.1159\n",
            "confusion: MAE: 0.5081, MSE: 0.4484, RMSE: 0.6696, R2 Score: 0.0455\n",
            "engagement: MAE: 0.5258, MSE: 0.4218, RMSE: 0.6495, R2 Score: -0.1033\n",
            "frustration: MAE: 0.4387, MSE: 0.3549, RMSE: 0.5957, R2 Score: 0.0114\n",
            "\n",
            "Epoch 2/2\n",
            "------------------------------\n",
            "Train Loss: 0.5383 | Val Loss: 0.4680 | Val MAE: 0.5219 | Val MSE: 0.4680 | Val RMSE: 0.6841 | Val R2: 0.0556 \n",
            " \n",
            "boredom: MAE: 0.6922, MSE: 0.6904, RMSE: 0.8309, R2 Score: 0.1530\n",
            "confusion: MAE: 0.4804, MSE: 0.4415, RMSE: 0.6645, R2 Score: 0.0602\n",
            "engagement: MAE: 0.5175, MSE: 0.4135, RMSE: 0.6430, R2 Score: -0.0815\n",
            "frustration: MAE: 0.3974, MSE: 0.3264, RMSE: 0.5713, R2 Score: 0.0906\n",
            "\n",
            "🕒 Entraînement terminé en 2m 43s\n"
          ]
        }
      ],
      "source": [
        "# --- 7. Lancement de l'entraînement ---\n",
        "print(\"\\nDébut de l'entraînement...\\n\")\n",
        "history = train_model(model=model_ft,\n",
        "                        criterion_emotions=criterion_emotions,\n",
        "                        optimizer=optimizer_ft,\n",
        "                        train_loader=train_loader,\n",
        "                        val_loader=val_loader,\n",
        "                        device=DEVICE,\n",
        "                        scheduler=exp_lr_scheduler,\n",
        "                        epochs=2,\n",
        "                        output_names=EMOTION_LABELS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IX/ Enregistrement model + export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Modèle exporté au format ONNX : daisee_model.onnx\n"
          ]
        }
      ],
      "source": [
        "# Chemin de sauvegarde\n",
        "onnx_export_path = \"daisee_model.onnx\"\n",
        "\n",
        "# Dummy input — doit correspondre à la taille attendue par ton modèle\n",
        "dummy_input = torch.randn(1, 3, 224, 224, device=DEVICE)\n",
        "\n",
        "# Export ONNX\n",
        "torch.onnx.export(\n",
        "    model_ft,                  # Le modèle entraîné\n",
        "    dummy_input,               # Un exemple d'input\n",
        "    onnx_export_path,          # Chemin de sortie\n",
        "    input_names=['input'],     # Nom de l'input\n",
        "    output_names=['output'],   # Nom de la sortie\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'output': {0: 'batch_size'}\n",
        "    },\n",
        "    opset_version=11,          # Version de l'opset ONNX (11 est sûr pour la compatibilité)\n",
        "    do_constant_folding=True   # Optimisation pour les constantes\n",
        ")\n",
        "\n",
        "print(f\"✅ Modèle exporté au format ONNX : {onnx_export_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zOnRjwFUhgo"
      },
      "outputs": [],
      "source": [
        "# # --- 8. Sauvegarde du modèle entraîné ---\n",
        "# # Créez un dossier pour les modèles si n'existe pas\n",
        "# os.makedirs('saved_models', exist_ok=True)\n",
        "# model_save_path = os.path.join('saved_models', 'resnet18_emotion_dav_multi_person.pth')\n",
        "# torch.save(model_ft.state_dict(), model_save_path)\n",
        "# print(f\"Modèle sauvegardé à : {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AR5Dl7-GCrI"
      },
      "outputs": [],
      "source": [
        "# --- 9. (Optionnel) Évaluation finale sur l'ensemble de validation ---\n",
        "# Charger le modèle pour l'évaluation\n",
        "# model_ft.eval() # Mettre en mode évaluation\n",
        "# ... Vous pouvez réutiliser le code d'évaluation de la fonction train_model ici si vous voulez une évaluation finale séparée.\n",
        "\n",
        "print(\"\\n--- Entraînement terminé ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import onnxruntime as ort\n",
        "# import numpy as np\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "\n",
        "# # Charger le modèle ONNX\n",
        "# session = ort.InferenceSession(\"emotic_model.onnx\")\n",
        "\n",
        "# # Charger et prétraiter une image\n",
        "# img_path = \"chemin/vers/image.jpg\"\n",
        "# image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                          [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# input_tensor = transform(image).unsqueeze(0).numpy()  # (1, 3, 224, 224)\n",
        "\n",
        "# # Faire une prédiction\n",
        "# outputs = session.run(['output'], {'input': input_tensor})\n",
        "# preds = outputs[0]\n",
        "\n",
        "# # Résultat\n",
        "# print(\"Prédiction brute :\", preds)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl_project_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
