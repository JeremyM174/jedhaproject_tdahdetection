{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMTE-gy8QrZx"
      },
      "source": [
        "**Étapes pour utiliser le code :**\n",
        "\n",
        "1. Préparez votre dataset comme décrit, avec les images dans un dossier et le fichier CSV des labels.\n",
        "\n",
        "2. Mettez à jour les variables DATA_DIR, CSV_FILE, IMAGE_DIR, et surtout la liste EMOTION_LABELS pour qu'elle corresponde exactement à vos émotions et l'ordre de vos colonnes dans le CSV.\n",
        "\n",
        "3. Exécutez le script.\n",
        "\n",
        "4. Surveillez la console pour les métriques d'entraînement et de validation. Ajustez les hyperparamètres (BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, FREEZE_FEATURES) si nécessaire en fonction des performances observées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLKa9aKKRG7N"
      },
      "source": [
        "**Organisation des données :**\n",
        "\n",
        "your_dataset/\n",
        "\n",
        "├── images/\n",
        "\n",
        "│   ├── image_001.jpg\n",
        "\n",
        "│   ├── image_002.jpg\n",
        "\n",
        "│   └── ...\n",
        "\n",
        "└── labels.csv # Ou un autre format de fichier, contenant les chemins d'images et leurs étiquettes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD1t0ktwWJ0U"
      },
      "source": [
        "**Format du dataset :**\n",
        "\n",
        "![Capture d'écran 2025-07-21 161638.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q2GLYWLdRDFO"
      },
      "outputs": [],
      "source": [
        "# pip install torch torchvision torchaudio\n",
        "# pip install matplotlib scikit-learn pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "17t4rr1xS1ja"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models#, transforms\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, mean_squared_error\n",
        "import numpy as np\n",
        "from tqdm import tqdm # Pour les barres de progression\n",
        "\n",
        "#import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_vOu_5EUt30"
      },
      "source": [
        "On a le choix de fine tuner la dernière couche OU tout le modele (pas le même temps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# I/ Configuration et Hyperparamètres "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b1KN8LGLS6gP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilisation du périphérique : cuda:0\n"
          ]
        }
      ],
      "source": [
        "# --- 0. Configuration et Hyperparamètres ---\n",
        "DATA_DIR = r'C:\\Users\\alber\\Desktop\\visual_studio_code\\dossier_jedha\\Jedha_Full_stack\\00_Final_Project\\emotic' # Chemin vers votre dossier racine du dataset\n",
        "CSV_FILE = os.path.join(DATA_DIR, 'labels.csv') # Chemin vers votre fichier CSV d'étiquettes\n",
        "IMAGE_DIR = os.path.join(DATA_DIR, 'images') # Chemin vers le dossier contenant les images\n",
        "\n",
        "BODY_BBOX_COLS = ['body_bbox_x1', 'body_bbox_y1', 'body_bbox_x2', 'body_bbox_y2'] # Noms de vos colonnes de face_box dans le CSV\n",
        "\n",
        "# Définissez vos émotions binaires ici, dans le même ordre que vos colonnes dans le CSV\n",
        "EMOTION_LABELS = [\n",
        "    'Disconnection',\n",
        "    'Doubt/Confusion',\n",
        "    'Fatigue',\n",
        "    'Pain',\n",
        "    'Disquietment',\n",
        "    'Annoyance',\n",
        "    'others',\n",
        "    'adhd_emotion'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(EMOTION_LABELS)\n",
        "\n",
        "# Définissez vos labels continus (DAV) ici\n",
        "CONTINUOUS_LABELS = ['arousal', 'valence','dominance']\n",
        "NUM_CONTINUOUS_CLASSES = len(CONTINUOUS_LABELS) # Sera 3\n",
        "IMAGE_DIM_COLS = ['height', 'width'] # Si vous utilisez ces colonnes pour le prétraitement\n",
        "\n",
        "BATCH_SIZE = 64 #32\n",
        "NUM_EPOCHS = 2 #20\n",
        "LEARNING_RATE = 0.002 #0.001\n",
        "FREEZE_FEATURES = True # True pour ne fine-tuner que la dernière couche, False pour fine-tuner tout le modèle\n",
        "\n",
        "# Pondération des pertes : Vous pouvez ajuster ces valeurs si une tâche est plus importante que l'autre\n",
        "LOSS_WEIGHT_EMOTIONS = 1.0\n",
        "LOSS_WEIGHT_CONTINUOUS = 1.0\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation du périphérique : {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(DATA_DIR)\n",
        "# print(CSV_FILE)\n",
        "# print(IMAGE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# II/ Création du Dataset PyTorch personnalisé (modifié)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7i1HiG6PTReS"
      },
      "outputs": [],
      "source": [
        "# --- 1. Création du Dataset PyTorch personnalisé (modifié) ---\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, emotion_cols, continuous_cols, bbox_cols, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.emotion_cols = emotion_cols\n",
        "        self.continuous_cols = continuous_cols\n",
        "        self.bbox_cols = bbox_cols # Nouvelle ligne pour les colonnes de la bbox\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "\n",
        "        emotion_labels = row[self.emotion_cols].values.astype(float)\n",
        "        emotion_labels = torch.tensor(emotion_labels, dtype=torch.float32)\n",
        "        continuous_labels = row[self.continuous_cols].values.astype(float)\n",
        "        continuous_labels = torch.tensor(continuous_labels, dtype=torch.float32)\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        x1, y1, x2, y2 = row[self.bbox_cols].values.astype(int)\n",
        "        cropped_image = image.crop((x1, y1, x2, y2))\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(cropped_image)\n",
        "\n",
        "        return image, emotion_labels, continuous_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# III/ Transformations d'images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LkV1CWUbTUJB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\anaconda3\\envs\\dl_project_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Transformations d'images ---\n",
        "# Transformations pour l'entraînement (augmentation des données)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RGB(),\n",
        "        transforms.RandomResizedCrop(224), # Recadrage aléatoire et redimensionnement à 224x224\n",
        "        transforms.RandomHorizontalFlip(), # Retournement horizontal aléatoire\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Jitter de couleur\n",
        "        transforms.ToTensor(), # Convertir en Tensor PyTorch (met les pixels entre 0 et 1)\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalisation (moyenne et std ImageNet)\n",
        "    ]),\n",
        "    # Transformations pour la validation/test (juste redimensionnement et normalisation)\n",
        "    'val': transforms.Compose([\n",
        "        transforms.RGB(),\n",
        "        transforms.Resize(256), # Redimensionner le plus petit côté à 256\n",
        "        transforms.CenterCrop(224), # Centre Crop à 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IV/ Chargement des données et division Train/Val "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-mNFgbICTrGD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement des données...\n",
            "Taille du dataset d'entraînement : 13516\n",
            "Taille du dataset de validation : 3380\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Chargement des données et division Train/Val ---\n",
        "print(\"Chargement des données...\")\n",
        "full_df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# Division du dataset en ensembles d'entraînement et de validation\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df['adhd_emotion'] if True else None) # Stratify si possible\n",
        "\n",
        "train_dataset = EmotionDataset(df=train_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, continuous_cols=CONTINUOUS_LABELS, bbox_cols=BODY_BBOX_COLS, transform=data_transforms['train'])\n",
        "val_dataset = EmotionDataset(df=val_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, continuous_cols=CONTINUOUS_LABELS, bbox_cols=BODY_BBOX_COLS, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count() // 2 or 1) # Utilisez des workers si > 1 CPU\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count() // 2 or 1)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "print(f\"Taille du dataset d'entraînement : {dataset_sizes['train']}\")\n",
        "print(f\"Taille du dataset de validation : {dataset_sizes['val']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i in range (10):\n",
        "#     print(train_dataset[i][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V/ Chargement du modèle ResNet18 pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDuN1GPuUDgX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement et modification du modèle ResNet18 pré-entraîné...\n"
          ]
        }
      ],
      "source": [
        "# --- 4. Chargement du modèle ResNet18 pré-entraîné ---\n",
        "print(\"Chargement et modification du modèle ResNet18 pré-entraîné...\")\n",
        "\n",
        "class MultiTaskResNet(nn.Module):\n",
        "    def __init__(self, num_emotion_classes, num_continuous_classes):\n",
        "        super(MultiTaskResNet, self).__init__()\n",
        "        # Charger ResNet18 pré-entraîné\n",
        "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Obtenir le nombre de caractéristiques de la couche fc avant qu'elle ne soit remplacée\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "\n",
        "        # Supprimer la dernière couche fc par défaut\n",
        "        self.resnet.fc = nn.Identity() # Remplace fc par une couche d'identité pour obtenir les features avant la classification\n",
        "\n",
        "        # Tête de classification pour les émotions (multi-label)\n",
        "        self.emotion_head = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512), # Couche cachée optionnelle\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_emotion_classes) # Sortie pour les émotions\n",
        "        )\n",
        "\n",
        "        # Tête de régression pour dominance, arousal, valence\n",
        "        self.continuous_head = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512), # Couche cachée optionnelle\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_continuous_classes) # Sortie pour DAV\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.resnet(x) # Passer l'image à travers ResNet pour obtenir les caractéristiques\n",
        "        emotion_output = self.emotion_head(features) # Passer les caractéristiques à la tête des émotions\n",
        "        continuous_output = self.continuous_head(features) # Passer les caractéristiques à la tête DAV\n",
        "        return emotion_output, continuous_output\n",
        "\n",
        "model_ft = MultiTaskResNet(NUM_CLASSES, NUM_CONTINUOUS_CLASSES)\n",
        "model_ft = model_ft.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# # Print model summary\n",
        "# summary(model_ft)  # (batch_size, input_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VI/ Fonction de perte et Optimiseur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PAV_C7MYUH2A"
      },
      "outputs": [],
      "source": [
        "# --- 5. Fonction de perte et Optimiseur ---\n",
        "# Pour la classification multi-label, Binary Cross-Entropy with Logits Loss est la norme.\n",
        "# Elle combine une couche Sigmoid et la Binary Cross-Entropy.\n",
        "# Perte pour la classification multi-label des émotions\n",
        "criterion_emotions = nn.BCEWithLogitsLoss()\n",
        "# Perte pour la régression des valeurs continues (DAV)\n",
        "criterion_continuous = nn.MSELoss() # Mean Squared Error Loss est standard pour la régression\n",
        "\n",
        "# Seulement les paramètres qui nécessitent des gradients seront optimisés\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Ordonnanceur de taux d'apprentissage (réduit le LR après un certain nombre d'époques)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VII/ Fonction d'entraînement et d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vPYQrVNKUdzA"
      },
      "outputs": [],
      "source": [
        "# --- 6. Fonction d'entraînement et d'évaluation ---\n",
        "def train_model(model, criterion_emotions, criterion_continuous, optimizer, loss_w_emotion, loss_w_continious, epochs=2):\n",
        "    since = time.time()\n",
        "\n",
        "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch}/{epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_loss_emotions = 0.0\n",
        "        total_loss_continuous = 0.0\n",
        "        correct = 0\n",
        "        correct_emotions = 0\n",
        "        correct_continuous = 0\n",
        "\n",
        "        for inputs, emotion_labels, continuous_labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            emotion_outputs, continuous_outputs = model(inputs)\n",
        "            loss_emotions = criterion_emotions(emotion_outputs, emotion_labels)\n",
        "            loss_continuous = criterion_continuous(continuous_outputs, continuous_labels)\n",
        "\n",
        "            # Perte totale avec pondération\n",
        "            loss = loss_w_emotion * loss_emotions + loss_w_continious * loss_continuous\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistiques\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            total_loss_emotions += loss_emotions.item() * inputs.size(0)\n",
        "            total_loss_continuous += loss_continuous.item() * inputs.size(0)\n",
        "            correct_emotions += (torch.argmax(emotion_outputs, dim=1) == emotion_labels).sum().item()  # Count correct predictions\n",
        "            correct_continuous += (torch.argmax(continuous_outputs, dim=1) == continuous_labels).sum().item()  # Count correct predictions\n",
        "            correct += correct_emotions + correct_continuous\n",
        "\n",
        "            # Pour les émotions : convertir les logits en prédictions binaires\n",
        "            # emotion_probs = torch.sigmoid(emotion_outputs)\n",
        "            # emotion_preds = (emotion_probs > 0.5).float() # Seuil de 0.5\n",
        "\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_loss_emotions = total_loss_emotions / len(train_loader)\n",
        "        train_loss_continuous = total_loss_continuous / len(train_loader)\n",
        "        train_acc = correct/len(train_loader)\n",
        "        train_emotion_acc = correct_continuous/len(train_loader)\n",
        "        train_continuous_acc = correct_continuous/len(train_loader)\n",
        "\n",
        "           \n",
        "        # Sauvegarder le meilleur modèle basé sur le F1-Micro des émotions ou un score combiné\n",
        "        # Pour une tâche multi-objectifs, il faut décider quelle métrique est la plus importante.\n",
        "        # Ici, on priorise le F1-Micro des émotions.\n",
        "        model.eval()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_loss_emotions = 0.0\n",
        "        val_loss_continuous = 0.0\n",
        "        val_correct = 0\n",
        "        val_correct_emotions = 0\n",
        "        val_correct_continuous = 0\n",
        "\n",
        "        with torch.no_grad():  # No need to compute gradients during validation\n",
        "            for inputs, emotion_labels, continuous_labels in val_loader:\n",
        "                emotion_outputs, continuous_outputs = model(inputs)\n",
        "                val_loss_emotions = criterion_emotions(emotion_outputs, emotion_labels)\n",
        "                val_loss_continuous = criterion_continuous(continuous_outputs, continuous_labels)\n",
        "                val_loss = loss_w_emotion * val_loss_emotions + loss_w_continious * val_loss_continuous\n",
        "                val_loss += val_loss.item() * inputs.size(0)\n",
        "                val_loss_emotions += val_loss_emotions.item() * inputs.size(0)\n",
        "                val_loss_continuous += val_loss_continuous.item() * inputs.size(0)\n",
        "                val_correct_emotions += (torch.argmax(emotion_outputs, dim=1) == emotion_labels).sum().item()  # Count correct predictions\n",
        "                val_correct_continuous += (torch.argmax(continuous_outputs, dim=1) == continuous_labels).sum().item()  # Count correct predictions\n",
        "                val_correct += val_correct_emotions + val_correct_continuous\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_loss_emotions = val_loss_emotions / len(val_loader)\n",
        "        val_loss_continuous = val_loss_continuous / len(val_loader)\n",
        "        val_acc = val_correct/len(val_loader)\n",
        "        val_emotion_acc = val_correct_continuous/len(val_loader)\n",
        "        val_continuous_acc = val_correct_continuous/len(val_loader)\n",
        "\n",
        "        # Store metrics in history dictionary\n",
        "        history['loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['accuracy'].append(train_acc)\n",
        "        history['val_accuracy'].append(val_acc)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Entraînement complet en {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/1\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "history = train_model(model_ft, \n",
        "                      criterion_emotions, \n",
        "                      criterion_continuous, \n",
        "                      optimizer=optimizer_ft, \n",
        "                      loss_w_emotion=LOSS_WEIGHT_EMOTIONS, \n",
        "                      loss_w_continious=LOSS_WEIGHT_CONTINUOUS, \n",
        "                      epochs=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for inputs, emotion_labels, continuous_labels in dataloaders['train']:\n",
        "#     inputs = inputs.to(DEVICE)\n",
        "#     emotion_labels = emotion_labels.to(DEVICE)\n",
        "#     continuous_labels = continuous_labels.to(DEVICE)\n",
        "#     print(inputs.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_image = torch.rand(64, 3, 224, 224).to('cuda')\n",
        "# model_ft = model_ft.to('cuda')\n",
        "# result = model_ft(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VIII/ Lancement de l'entraînement "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o26pD-BuUfA6"
      },
      "outputs": [],
      "source": [
        "# --- 7. Lancement de l'entraînement ---\n",
        "print(\"\\nDébut de l'entraînement...\")\n",
        "model_ft = train_model(model_ft, criterion_emotions, criterion_continuous, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zOnRjwFUhgo"
      },
      "outputs": [],
      "source": [
        "# --- 8. Sauvegarde du modèle entraîné ---\n",
        "# Créez un dossier pour les modèles si n'existe pas\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "model_save_path = os.path.join('saved_models', 'resnet18_emotion_dav_multi_person.pth')\n",
        "torch.save(model_ft.state_dict(), model_save_path)\n",
        "print(f\"Modèle sauvegardé à : {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AR5Dl7-GCrI"
      },
      "outputs": [],
      "source": [
        "# --- 9. (Optionnel) Évaluation finale sur l'ensemble de validation ---\n",
        "# Charger le modèle pour l'évaluation\n",
        "# model_ft.eval() # Mettre en mode évaluation\n",
        "# ... Vous pouvez réutiliser le code d'évaluation de la fonction train_model ici si vous voulez une évaluation finale séparée.\n",
        "\n",
        "print(\"\\n--- Entraînement terminé ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl_project_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
