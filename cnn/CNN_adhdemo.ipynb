{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMTE-gy8QrZx"
      },
      "source": [
        "**√âtapes pour utiliser le code :**\n",
        "\n",
        "1. Pr√©parez votre dataset comme d√©crit, avec les images dans un dossier et le fichier CSV des labels.\n",
        "\n",
        "2. Mettez √† jour les variables DATA_DIR, CSV_FILE, IMAGE_DIR, et surtout la liste EMOTION_LABELS pour qu'elle corresponde exactement √† vos √©motions et l'ordre de vos colonnes dans le CSV.\n",
        "\n",
        "3. Ex√©cutez le script.\n",
        "\n",
        "4. Surveillez la console pour les m√©triques d'entra√Ænement et de validation. Ajustez les hyperparam√®tres (BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, FREEZE_FEATURES) si n√©cessaire en fonction des performances observ√©es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLKa9aKKRG7N"
      },
      "source": [
        "**Organisation des donn√©es :**\n",
        "\n",
        "your_dataset/\n",
        "\n",
        "‚îú‚îÄ‚îÄ images/\n",
        "\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ image_001.jpg\n",
        "\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ image_002.jpg\n",
        "\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "\n",
        "‚îî‚îÄ‚îÄ labels.csv # Ou un autre format de fichier, contenant les chemins d'images et leurs √©tiquettes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD1t0ktwWJ0U"
      },
      "source": [
        "**Format du dataset :**\n",
        "\n",
        "![Capture d'√©cran 2025-07-21 161638.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17t4rr1xS1ja"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models#, transforms\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import onnx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm # Pour les barres de progression\n",
        "\n",
        "#import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_vOu_5EUt30"
      },
      "source": [
        "On a le choix de fine tuner la derni√®re couche OU tout le modele (pas le m√™me temps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# I/ Configuration et Hyperparam√®tres "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b1KN8LGLS6gP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilisation du p√©riph√©rique : cuda:0\n"
          ]
        }
      ],
      "source": [
        "# --- 0. Configuration et Hyperparam√®tres ---\n",
        "DATA_DIR = r'C:\\Users\\alber\\Desktop\\visual_studio_code\\dossier_jedha\\Jedha_Full_stack\\00_Final_Project\\emotic' # Chemin vers votre dossier racine du dataset\n",
        "CSV_FILE = os.path.join(DATA_DIR, 'labels_v2.csv') # Chemin vers votre fichier CSV d'√©tiquettes\n",
        "IMAGE_DIR = os.path.join(DATA_DIR, 'images') # Chemin vers le dossier contenant les images\n",
        "\n",
        "BODY_BBOX_COLS = ['body_bbox_x1', 'body_bbox_y1', 'body_bbox_x2', 'body_bbox_y2'] # Noms de vos colonnes de face_box dans le CSV\n",
        "\n",
        "# D√©finissez vos √©motions binaires ici, dans le m√™me ordre que vos colonnes dans le CSV\n",
        "EMOTION_LABELS = [\n",
        "    'Disconnection',\n",
        "    'Doubt/Confusion',\n",
        "    'Fatigue',\n",
        "    'Pain',\n",
        "    'Disquietment',\n",
        "    'Annoyance',\n",
        "    'others',\n",
        "    'adhd_emotion'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(EMOTION_LABELS)\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "FREEZE_FEATURES = True # True pour ne fine-tuner que la derni√®re couche, False pour fine-tuner tout le mod√®le\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation du p√©riph√©rique : {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(DATA_DIR)\n",
        "# print(CSV_FILE)\n",
        "# print(IMAGE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Charger le fichier CSV des annotations\n",
        "# df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# # V√©rifie qu'une colonne 'image' ou 'filename' existe\n",
        "# assert 'image' in df.columns or 'filename' in df.columns, \"‚ùå Le fichier CSV ne contient pas de colonne 'image' ou 'filename'.\"\n",
        "\n",
        "# # Nom de la colonne contenant les noms d'image\n",
        "# image_column = 'image' if 'image' in df.columns else 'filename'\n",
        "\n",
        "# # V√©rification de l'existence des fichiers image\n",
        "# missing_images = []\n",
        "# for img_name in df[image_column].unique():\n",
        "#     img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "#     if not os.path.isfile(img_path):\n",
        "#         missing_images.append(img_name)\n",
        "\n",
        "# # Affichage du r√©sultat\n",
        "# if missing_images:\n",
        "#     print(f\"‚ùå {len(missing_images)} image(s) manquante(s) sur {df[image_column].nunique()} :\")\n",
        "#     print(missing_images[:10])  # Affiche les 10 premi√®res si la liste est longue\n",
        "# else:\n",
        "#     print(\"‚úÖ Toutes les images r√©f√©renc√©es dans le CSV sont pr√©sentes dans le dossier images/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# II/ Cr√©ation du Dataset PyTorch personnalis√© (modifi√©)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7i1HiG6PTReS"
      },
      "outputs": [],
      "source": [
        "# --- 1. Cr√©ation du Dataset PyTorch personnalis√© (modifi√©) ---\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, emotion_cols, bbox_cols, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.emotion_cols = emotion_cols\n",
        "        self.bbox_cols = bbox_cols # Nouvelle ligne pour les colonnes de la bbox\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "\n",
        "        emotion_labels = row[self.emotion_cols].values.astype(float)\n",
        "        emotion_labels = torch.tensor(emotion_labels, dtype=torch.float32)\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        x1, y1, x2, y2 = row[self.bbox_cols].values.astype(int)\n",
        "        cropped_image = image.crop((x1, y1, x2, y2))\n",
        "        \n",
        "        if self.transform:\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        return cropped_image, emotion_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Chargement du CSV\n",
        "# df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# # Prendre une ligne al√©atoire\n",
        "# sample_row = df.sample(1).iloc[0]\n",
        "# img_name = sample_row['image']  # colonne contenant le nom du fichier image\n",
        "# img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "\n",
        "# print(f\"Test de lecture de l'image : {img_path}\")\n",
        "\n",
        "# # Mesure du temps de lecture\n",
        "# start = time.time()\n",
        "# try:\n",
        "#     img = Image.open(img_path).convert('RGB')\n",
        "#     img.load()  # Force le chargement complet\n",
        "#     print(f\"‚úÖ Image lue correctement. Taille = {img.size}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"‚ùå Erreur de lecture de l'image : {e}\")\n",
        "\n",
        "# end = time.time()\n",
        "# print(f\"‚è± Temps de lecture : {end - start:.4f} secondes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# III/ Transformations d'images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LkV1CWUbTUJB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\anaconda3\\envs\\dl_project_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Transformations d'images ---\n",
        "# Transformations pour l'entra√Ænement (augmentation des donn√©es)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224), # Recadrage al√©atoire et redimensionnement √† 224x224\n",
        "        transforms.RandomHorizontalFlip(), # Retournement horizontal al√©atoire\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Jitter de couleur\n",
        "        transforms.ToTensor(), # Convertir en Tensor PyTorch (met les pixels entre 0 et 1)\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalisation (moyenne et std ImageNet)\n",
        "    ]),\n",
        "    # Transformations pour la validation/test (juste redimensionnement et normalisation)\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256), # Redimensionner le plus petit c√¥t√© √† 256\n",
        "        transforms.CenterCrop(224), # Centre Crop √† 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IV/ Chargement des donn√©es et division Train/Val "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-mNFgbICTrGD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement des donn√©es...\n",
            "Taille du dataset d'entra√Ænement : 13661\n",
            "Taille du dataset de validation : 3416\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Chargement des donn√©es et division Train/Val ---\n",
        "print(\"Chargement des donn√©es...\")\n",
        "full_df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# Division du dataset en ensembles d'entra√Ænement et de validation\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df['adhd_emotion'] if True else None) # Stratify si possible\n",
        "\n",
        "train_dataset = EmotionDataset(df=train_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, bbox_cols=BODY_BBOX_COLS, transform=data_transforms['train'])\n",
        "val_dataset = EmotionDataset(df=val_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, bbox_cols=BODY_BBOX_COLS, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "print(f\"Taille du dataset d'entra√Ænement : {dataset_sizes['train']}\")\n",
        "print(f\"Taille du dataset de validation : {dataset_sizes['val']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_dataset[0]  # ou un autre index si √ßa bloque toujours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next(iter(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Test 0 : V√©rifie si le DataLoader donne quelque chose ---\n",
        "# print(\"Test du premier batch dans val_loader...\")\n",
        "\n",
        "# try:\n",
        "#     sample_batch = next(iter(val_loader))\n",
        "#     images, labels = sample_batch\n",
        "#     print(f\"Batch charg√© : {images.shape} - {labels.shape}\")\n",
        "# except Exception as e:\n",
        "#     print(\"Erreur pendant le chargement du batch :\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Test 1 : V√©rifie si le DataLoader donne quelque chose ---\n",
        "# print(\"Test du premier batch dans train_loader...\")\n",
        "\n",
        "# try:\n",
        "#     sample_batch = next(iter(train_loader))\n",
        "#     images, labels = sample_batch\n",
        "#     print(f\"Batch charg√© : {images.shape} - {labels.shape}\")\n",
        "# except Exception as e:\n",
        "#     print(\"Erreur pendant le chargement du batch :\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Active le DataLoader uniquement\n",
        "# print(\"‚è≥ Test de vitesse du train_loader...\")\n",
        "\n",
        "# n_batches_to_test = 20  # Tu peux ajuster selon la taille de ton dataset\n",
        "# total_load_time = 0.0\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# for i, (inputs, labels) in enumerate(train_loader):\n",
        "#     if i >= n_batches_to_test:\n",
        "#         break\n",
        "\n",
        "#     start = time.time()\n",
        "\n",
        "#     # Envoie au GPU (si utile pour tester l'impact)\n",
        "#     inputs = inputs.to(device)\n",
        "#     labels = labels.to(device)\n",
        "\n",
        "#     # Attendre que les donn√©es soient vraiment transf√©r√©es\n",
        "#     torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
        "\n",
        "#     end = time.time()\n",
        "#     load_time = end - start\n",
        "#     total_load_time += load_time\n",
        "#     print(f\"üß± Batch {i+1} - Temps de chargement + transfert : {load_time:.4f} sec\")\n",
        "\n",
        "# print(\"\\nüìä Moyenne sur\", n_batches_to_test, \"batches :\")\n",
        "# print(f\"‚è± Temps moyen par batch : {total_load_time / n_batches_to_test:.4f} sec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i in range (2):\n",
        "#     print(train_dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V/ Chargement du mod√®le ResNet18 pr√©-entra√Æn√©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement et modification du mod√®le ResNet18 pr√©-entra√Æn√©...\n",
            "Freeze des couches convolutionnelles...\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.bn1.bias\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.bn2.bias\n",
            "layer4.0.downsample.0.weight\n",
            "layer4.0.downsample.1.weight\n",
            "layer4.0.downsample.1.bias\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.bn1.bias\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "layer4.1.bn2.bias\n",
            "fc.weight\n",
            "fc.bias\n",
            "D√©finition de la derni√®re couche...\n",
            "Mod√®le pr√™t ! ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "print(\"Chargement et modification du mod√®le ResNet18 pr√©-entra√Æn√©...\")\n",
        "\n",
        "# 1. Chargement du mod√®le ResNet18 pr√©-entra√Æn√© sur ImageNet\n",
        "model_ft = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"Freeze des couches convolutionnelles...\")\n",
        "\n",
        "# 2. On g√®le les couches de feature extraction\n",
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#fine_tune_at = len(list(model_ft.children())) - 10\n",
        "for name, param in list(model_ft.named_parameters())[-17:]:\n",
        "    print(name)\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"D√©finition de la derni√®re couche...\")\n",
        "\n",
        "# 3. Remplacement de la couche fully-connected finale\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "model_ft.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(512, NUM_CLASSES)\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Envoi du mod√®le sur le bon device (GPU ou CPU)\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "\n",
        "print(\"Mod√®le pr√™t ! ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(model_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [1, 8]                    --\n",
              "‚îú‚îÄConv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
              "‚îú‚îÄBatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
              "‚îú‚îÄReLU: 1-3                              [1, 64, 112, 112]         --\n",
              "‚îú‚îÄMaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
              "‚îú‚îÄSequential: 1-5                        [1, 64, 56, 56]           --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [1, 64, 56, 56]           (36,864)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-3                    [1, 64, 56, 56]           --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                    [1, 64, 56, 56]           --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [1, 64, 56, 56]           (36,864)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-8             [1, 64, 56, 56]           (128)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-9                    [1, 64, 56, 56]           --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-10                 [1, 64, 56, 56]           (36,864)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-11            [1, 64, 56, 56]           (128)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-12                   [1, 64, 56, 56]           --\n",
              "‚îú‚îÄSequential: 1-6                        [1, 128, 28, 28]          --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-13                 [1, 128, 28, 28]          (73,728)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-14            [1, 128, 28, 28]          (256)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-15                   [1, 128, 28, 28]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-16                 [1, 128, 28, 28]          (147,456)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-17            [1, 128, 28, 28]          (256)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-18             [1, 128, 28, 28]          (8,448)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-19                   [1, 128, 28, 28]          --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-20                 [1, 128, 28, 28]          (147,456)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-21            [1, 128, 28, 28]          (256)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-22                   [1, 128, 28, 28]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-23                 [1, 128, 28, 28]          (147,456)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-24            [1, 128, 28, 28]          (256)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-25                   [1, 128, 28, 28]          --\n",
              "‚îú‚îÄSequential: 1-7                        [1, 256, 14, 14]          --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-26                 [1, 256, 14, 14]          (294,912)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-27            [1, 256, 14, 14]          (512)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-28                   [1, 256, 14, 14]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-29                 [1, 256, 14, 14]          (589,824)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-30            [1, 256, 14, 14]          (512)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-31             [1, 256, 14, 14]          (33,280)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-32                   [1, 256, 14, 14]          --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-33                 [1, 256, 14, 14]          (589,824)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-34            [1, 256, 14, 14]          (512)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-35                   [1, 256, 14, 14]          --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-36                 [1, 256, 14, 14]          (589,824)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-37            [1, 256, 14, 14]          (512)\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-38                   [1, 256, 14, 14]          --\n",
              "‚îú‚îÄSequential: 1-8                        [1, 512, 7, 7]            --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-41                   [1, 512, 7, 7]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-44             [1, 512, 7, 7]            132,096\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-45                   [1, 512, 7, 7]            --\n",
              "‚îÇ    ‚îî‚îÄBasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-48                   [1, 512, 7, 7]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-51                   [1, 512, 7, 7]            --\n",
              "‚îú‚îÄAdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
              "‚îú‚îÄSequential: 1-10                       [1, 8]                    --\n",
              "‚îÇ    ‚îî‚îÄLinear: 2-9                       [1, 512]                  262,656\n",
              "‚îÇ    ‚îî‚îÄBatchNorm1d: 2-10                 [1, 512]                  1,024\n",
              "‚îÇ    ‚îî‚îÄReLU: 2-11                        [1, 512]                  --\n",
              "‚îÇ    ‚îî‚îÄDropout: 2-12                     [1, 512]                  --\n",
              "‚îÇ    ‚îî‚îÄLinear: 2-13                      [1, 8]                    4,104\n",
              "==========================================================================================\n",
              "Total params: 11,444,296\n",
              "Trainable params: 8,661,512\n",
              "Non-trainable params: 2,782,784\n",
              "Total mult-adds (Units.GIGABYTES): 1.81\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 39.75\n",
              "Params size (MB): 45.78\n",
              "Estimated Total Size (MB): 86.13\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print model summary\n",
        "#summary(model_ft, input_size=(8, 3, 224, 224))  # (batch_size, input_features)\n",
        "summary(model_ft, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VI/ Fonction de perte et Optimiseur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16,\n",
            " 35,\n",
            " 40,\n",
            " 126,\n",
            " 46,\n",
            " 61,\n",
            " 1,\n",
            " 8\n"
          ]
        }
      ],
      "source": [
        "# definition des poids pour chaques emotions test 2  : \n",
        "\n",
        "Disconnection_w =  round(17077 / 1043)\n",
        "Doubt_Confusion_w = round(17077 / 489)\n",
        "Fatigue_w = round(17077 / 429)\n",
        "Pain_w = round(17077 / 135)\n",
        "Disquietment_w = round(17077 / 368)\n",
        "Annoyance_w = round(17077 / 278)\n",
        "Others_w = round(17077 / 16163)\n",
        "Adhd_emotion_w = round(17077 / 2211)\n",
        "\n",
        "print(f'{Disconnection_w},\\n {Doubt_Confusion_w},\\n {Fatigue_w},\\n {Pain_w},\\n {Disquietment_w},\\n {Annoyance_w},\\n {Others_w},\\n {Adhd_emotion_w}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_weights = torch.tensor([\n",
        "    Disconnection_w,  # Disconnection\n",
        "    Doubt_Confusion_w,  # Doubt/Confusion\n",
        "    Fatigue_w,  # Fatigue\n",
        "    Pain_w,  # Pain\n",
        "    Disquietment_w,  # Disquietment\n",
        "    Annoyance_w,  # Annoyance\n",
        "    Others_w,  # others\n",
        "    Adhd_emotion_w   # adhd_emotion\n",
        "], dtype=torch.float32).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(emotion_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.log2(emotion_weights*2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAV_C7MYUH2A"
      },
      "outputs": [],
      "source": [
        "# --- 5. Fonction de perte et Optimiseur ---\n",
        "# Pour la classification multi-label, Binary Cross-Entropy with Logits Loss est la norme.\n",
        "# Elle combine une couche Sigmoid et la Binary Cross-Entropy.\n",
        "# Perte pour la classification multi-label des √©motions\n",
        "\n",
        "criterion_emotions = nn.BCEWithLogitsLoss(pos_weight=torch.log2(emotion_weights*2))\n",
        "\n",
        "# Seulement les param√®tres qui n√©cessitent des gradients seront optimis√©s\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Ordonnanceur de taux d'apprentissage (r√©duit le LR apr√®s un certain nombre d'√©poques)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VII/ Fonction d'entra√Ænement et d'√©valuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Fonction d'entra√Ænement et d'√©valuation ---\n",
        "\n",
        "def train_model(model, criterion_emotions, optimizer, train_loader, val_loader, device, scheduler=None, epochs=NUM_EPOCHS):\n",
        "    \n",
        "    since = time.time()\n",
        "    \n",
        "    history = {'train_loss': [], 'val_loss': [],'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print('-' * 30)\n",
        "\n",
        "        ### -------- TRAIN --------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion_emotions(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Accuracy multi-label (threshold 0.5)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            running_corrects += (preds == labels).float().sum().item()\n",
        "            total_samples += labels.numel()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / total_samples\n",
        "\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "\n",
        "        ### -------- VAL --------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_emotions(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "                val_corrects += (preds == labels).float().sum().item()\n",
        "                val_total += labels.numel()\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_corrects / val_total\n",
        "\n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "        history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "        # Liste pour stocker toutes les pr√©dictions et les labels r√©els sur l'ensemble de validation\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_emotions(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                preds = (probs > 0.5).float()\n",
        "\n",
        "                val_corrects += (preds == labels).float().sum().item()\n",
        "                val_total += labels.numel()\n",
        "\n",
        "                all_preds.append(preds.cpu())\n",
        "                all_labels.append(labels.cpu())\n",
        "\n",
        "        # Concat√©nation de tous les batches\n",
        "        all_preds = torch.cat(all_preds).numpy()\n",
        "        all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "        # Affichage du rapport classification multilabel\n",
        "        print(\"\\nüìä Rapport de classification (Validation):\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=EMOTION_LABELS, zero_division=0))\n",
        "        \n",
        "        print(f\"Train Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
        "        print(f\"Val   Loss: {val_epoch_loss:.4f} | Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"\\nüïí Entra√Ænement termin√© en {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VIII/ Lancement de l'entra√Ænement "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "D√©but de l'entra√Ænement...\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.08      0.65      0.15       200\n",
            "Doubt/Confusion       0.04      0.57      0.08        98\n",
            "        Fatigue       0.04      0.77      0.07        88\n",
            "           Pain       0.01      0.58      0.02        31\n",
            "   Disquietment       0.03      0.36      0.06        75\n",
            "      Annoyance       0.02      0.60      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.17      0.62      0.27       442\n",
            "\n",
            "      micro avg       0.29      0.91      0.44      4216\n",
            "      macro avg       0.17      0.64      0.21      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.42      0.96      0.53      4216\n",
            "\n",
            "Train Loss: 1.3402 | Acc: 0.5589\n",
            "Val   Loss: 1.2257 | Acc: 0.6445\n",
            "\n",
            "Epoch 2/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.64      0.16       200\n",
            "Doubt/Confusion       0.05      0.53      0.09        98\n",
            "        Fatigue       0.05      0.73      0.09        88\n",
            "           Pain       0.02      0.48      0.03        31\n",
            "   Disquietment       0.04      0.61      0.07        75\n",
            "      Annoyance       0.02      0.74      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.17      0.71      0.28       442\n",
            "\n",
            "      micro avg       0.30      0.92      0.46      4216\n",
            "      macro avg       0.17      0.68      0.22      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.44      0.96      0.54      4216\n",
            "\n",
            "Train Loss: 1.2001 | Acc: 0.6135\n",
            "Val   Loss: 1.1976 | Acc: 0.6605\n",
            "\n",
            "Epoch 3/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.59      0.16       200\n",
            "Doubt/Confusion       0.05      0.45      0.09        98\n",
            "        Fatigue       0.04      0.66      0.08        88\n",
            "           Pain       0.02      0.61      0.04        31\n",
            "   Disquietment       0.04      0.45      0.07        75\n",
            "      Annoyance       0.02      0.57      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.65      0.29       442\n",
            "\n",
            "      micro avg       0.32      0.91      0.48      4216\n",
            "      macro avg       0.17      0.62      0.22      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.47      0.96      0.57      4216\n",
            "\n",
            "Train Loss: 1.1849 | Acc: 0.6266\n",
            "Val   Loss: 1.1946 | Acc: 0.6920\n",
            "\n",
            "Epoch 4/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.08      0.70      0.14       200\n",
            "Doubt/Confusion       0.04      0.59      0.08        98\n",
            "        Fatigue       0.05      0.61      0.09        88\n",
            "           Pain       0.02      0.39      0.03        31\n",
            "   Disquietment       0.04      0.63      0.07        75\n",
            "      Annoyance       0.02      0.58      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.17      0.67      0.27       442\n",
            "\n",
            "      micro avg       0.30      0.92      0.45      4216\n",
            "      macro avg       0.17      0.65      0.21      4216\n",
            "   weighted avg       0.75      0.92      0.78      4216\n",
            "    samples avg       0.46      0.96      0.55      4216\n",
            "\n",
            "Train Loss: 1.1879 | Acc: 0.6276\n",
            "Val   Loss: 1.2165 | Acc: 0.6551\n",
            "\n",
            "Epoch 5/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.08      0.65      0.14       200\n",
            "Doubt/Confusion       0.05      0.53      0.09        98\n",
            "        Fatigue       0.05      0.72      0.09        88\n",
            "           Pain       0.02      0.61      0.04        31\n",
            "   Disquietment       0.03      0.53      0.06        75\n",
            "      Annoyance       0.02      0.47      0.03        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.69      0.28       442\n",
            "\n",
            "      micro avg       0.30      0.92      0.45      4216\n",
            "      macro avg       0.17      0.65      0.21      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.47      0.96      0.56      4216\n",
            "\n",
            "Train Loss: 1.1766 | Acc: 0.6355\n",
            "Val   Loss: 1.2153 | Acc: 0.6563\n",
            "\n",
            "Epoch 6/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.61      0.15       200\n",
            "Doubt/Confusion       0.04      0.56      0.08        98\n",
            "        Fatigue       0.05      0.75      0.09        88\n",
            "           Pain       0.02      0.42      0.03        31\n",
            "   Disquietment       0.03      0.57      0.05        75\n",
            "      Annoyance       0.02      0.66      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.68      0.28       442\n",
            "\n",
            "      micro avg       0.29      0.92      0.45      4216\n",
            "      macro avg       0.17      0.66      0.21      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.44      0.96      0.54      4216\n",
            "\n",
            "Train Loss: 1.1769 | Acc: 0.6324\n",
            "Val   Loss: 1.2176 | Acc: 0.6489\n",
            "\n",
            "Epoch 7/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.10      0.54      0.16       200\n",
            "Doubt/Confusion       0.04      0.63      0.08        98\n",
            "        Fatigue       0.05      0.77      0.10        88\n",
            "           Pain       0.02      0.58      0.04        31\n",
            "   Disquietment       0.04      0.57      0.07        75\n",
            "      Annoyance       0.03      0.43      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.20      0.59      0.30       442\n",
            "\n",
            "      micro avg       0.33      0.90      0.49      4216\n",
            "      macro avg       0.18      0.64      0.22      4216\n",
            "   weighted avg       0.75      0.90      0.79      4216\n",
            "    samples avg       0.49      0.95      0.59      4216\n",
            "\n",
            "Train Loss: 1.1657 | Acc: 0.6318\n",
            "Val   Loss: 1.1693 | Acc: 0.7047\n",
            "\n",
            "Epoch 8/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.10      0.66      0.17       200\n",
            "Doubt/Confusion       0.05      0.61      0.09        98\n",
            "        Fatigue       0.05      0.72      0.10        88\n",
            "           Pain       0.02      0.45      0.03        31\n",
            "   Disquietment       0.04      0.59      0.07        75\n",
            "      Annoyance       0.02      0.43      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.66      0.29       442\n",
            "\n",
            "      micro avg       0.33      0.91      0.49      4216\n",
            "      macro avg       0.18      0.64      0.22      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.51      0.96      0.60      4216\n",
            "\n",
            "Train Loss: 1.1327 | Acc: 0.6521\n",
            "Val   Loss: 1.1516 | Acc: 0.7009\n",
            "\n",
            "Epoch 9/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.10      0.67      0.17       200\n",
            "Doubt/Confusion       0.05      0.62      0.09        98\n",
            "        Fatigue       0.05      0.73      0.10        88\n",
            "           Pain       0.02      0.52      0.04        31\n",
            "   Disquietment       0.04      0.59      0.07        75\n",
            "      Annoyance       0.02      0.45      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.67      0.28       442\n",
            "\n",
            "      micro avg       0.32      0.92      0.48      4216\n",
            "      macro avg       0.18      0.66      0.22      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.51      0.96      0.59      4216\n",
            "\n",
            "Train Loss: 1.1126 | Acc: 0.6581\n",
            "Val   Loss: 1.1479 | Acc: 0.6913\n",
            "\n",
            "Epoch 10/10\n",
            "------------------------------\n",
            "\n",
            "üìä Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.68      0.16       200\n",
            "Doubt/Confusion       0.05      0.61      0.09        98\n",
            "        Fatigue       0.05      0.62      0.10        88\n",
            "           Pain       0.02      0.45      0.04        31\n",
            "   Disquietment       0.04      0.57      0.07        75\n",
            "      Annoyance       0.02      0.38      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.66      0.29       442\n",
            "\n",
            "      micro avg       0.34      0.91      0.49      4216\n",
            "      macro avg       0.18      0.62      0.22      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.52      0.96      0.61      4216\n",
            "\n",
            "Train Loss: 1.1076 | Acc: 0.6593\n",
            "Val   Loss: 1.1536 | Acc: 0.7085\n",
            "\n",
            "üïí Entra√Ænement termin√© en 39m 34s\n"
          ]
        }
      ],
      "source": [
        "# --- 7. Lancement de l'entra√Ænement ---\n",
        "print(\"\\nD√©but de l'entra√Ænement...\\n\")\n",
        "history = train_model(model=model_ft,\n",
        "                        criterion_emotions=criterion_emotions,\n",
        "                        optimizer=optimizer_ft,\n",
        "                        train_loader=train_loader,\n",
        "                        val_loader=val_loader,\n",
        "                        device=DEVICE,\n",
        "                        scheduler=exp_lr_scheduler,\n",
        "                        epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IX/ Enregistrement model + export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chemin de sauvegarde\n",
        "onnx_export_path = \"emotic_model.onnx\"\n",
        "\n",
        "# Dummy input ‚Äî doit correspondre √† la taille attendue par ton mod√®le\n",
        "dummy_input = torch.randn(1, 3, 224, 224, device=DEVICE)\n",
        "\n",
        "# Export ONNX\n",
        "torch.onnx.export(\n",
        "    model_ft,                  # Le mod√®le entra√Æn√©\n",
        "    dummy_input,               # Un exemple d'input\n",
        "    onnx_export_path,          # Chemin de sortie\n",
        "    input_names=['input'],     # Nom de l'input\n",
        "    output_names=['output'],   # Nom de la sortie\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'output': {0: 'batch_size'}\n",
        "    },\n",
        "    opset_version=11,          # Version de l'opset ONNX (11 est s√ªr pour la compatibilit√©)\n",
        "    do_constant_folding=True   # Optimisation pour les constantes\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Mod√®le export√© au format ONNX : {onnx_export_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zOnRjwFUhgo"
      },
      "outputs": [],
      "source": [
        "# # --- 8. Sauvegarde du mod√®le entra√Æn√© ---\n",
        "# # Cr√©ez un dossier pour les mod√®les si n'existe pas\n",
        "# os.makedirs('saved_models', exist_ok=True)\n",
        "# model_save_path = os.path.join('saved_models', 'resnet18_emotion_dav_multi_person.pth')\n",
        "# torch.save(model_ft.state_dict(), model_save_path)\n",
        "# print(f\"Mod√®le sauvegard√© √† : {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AR5Dl7-GCrI"
      },
      "outputs": [],
      "source": [
        "# --- 9. (Optionnel) √âvaluation finale sur l'ensemble de validation ---\n",
        "# Charger le mod√®le pour l'√©valuation\n",
        "# model_ft.eval() # Mettre en mode √©valuation\n",
        "# ... Vous pouvez r√©utiliser le code d'√©valuation de la fonction train_model ici si vous voulez une √©valuation finale s√©par√©e.\n",
        "\n",
        "print(\"\\n--- Entra√Ænement termin√© ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import onnxruntime as ort\n",
        "# import numpy as np\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "\n",
        "# # Charger le mod√®le ONNX\n",
        "# session = ort.InferenceSession(\"emotic_model.onnx\")\n",
        "\n",
        "# # Charger et pr√©traiter une image\n",
        "# img_path = \"chemin/vers/image.jpg\"\n",
        "# image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                          [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# input_tensor = transform(image).unsqueeze(0).numpy()  # (1, 3, 224, 224)\n",
        "\n",
        "# # Faire une pr√©diction\n",
        "# outputs = session.run(['output'], {'input': input_tensor})\n",
        "# preds = outputs[0]\n",
        "\n",
        "# # R√©sultat\n",
        "# print(\"Pr√©diction brute :\", preds)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl_project_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
