{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMTE-gy8QrZx"
      },
      "source": [
        "**Étapes pour utiliser le code :**\n",
        "\n",
        "1. Préparez votre dataset comme décrit, avec les images dans un dossier et le fichier CSV des labels.\n",
        "\n",
        "2. Mettez à jour les variables DATA_DIR, CSV_FILE, IMAGE_DIR, et surtout la liste EMOTION_LABELS pour qu'elle corresponde exactement à vos émotions et l'ordre de vos colonnes dans le CSV.\n",
        "\n",
        "3. Exécutez le script.\n",
        "\n",
        "4. Surveillez la console pour les métriques d'entraînement et de validation. Ajustez les hyperparamètres (BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, FREEZE_FEATURES) si nécessaire en fonction des performances observées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLKa9aKKRG7N"
      },
      "source": [
        "**Organisation des données :**\n",
        "\n",
        "your_dataset/\n",
        "\n",
        "├── images/\n",
        "\n",
        "│   ├── image_001.jpg\n",
        "\n",
        "│   ├── image_002.jpg\n",
        "\n",
        "│   └── ...\n",
        "\n",
        "└── labels.csv # Ou un autre format de fichier, contenant les chemins d'images et leurs étiquettes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD1t0ktwWJ0U"
      },
      "source": [
        "**Format du dataset :**\n",
        "\n",
        "![Capture d'écran 2025-07-21 161638.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17t4rr1xS1ja"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models#, transforms\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import onnx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm # Pour les barres de progression\n",
        "\n",
        "#import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_vOu_5EUt30"
      },
      "source": [
        "On a le choix de fine tuner la dernière couche OU tout le modele (pas le même temps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# I/ Configuration et Hyperparamètres "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b1KN8LGLS6gP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilisation du périphérique : cuda:0\n"
          ]
        }
      ],
      "source": [
        "# --- 0. Configuration et Hyperparamètres ---\n",
        "DATA_DIR = r'C:\\Users\\alber\\Desktop\\visual_studio_code\\dossier_jedha\\Jedha_Full_stack\\00_Final_Project\\emotic' # Chemin vers votre dossier racine du dataset\n",
        "CSV_FILE = os.path.join(DATA_DIR, 'labels_v2.csv') # Chemin vers votre fichier CSV d'étiquettes\n",
        "IMAGE_DIR = os.path.join(DATA_DIR, 'images') # Chemin vers le dossier contenant les images\n",
        "\n",
        "BODY_BBOX_COLS = ['body_bbox_x1', 'body_bbox_y1', 'body_bbox_x2', 'body_bbox_y2'] # Noms de vos colonnes de face_box dans le CSV\n",
        "\n",
        "# Définissez vos émotions binaires ici, dans le même ordre que vos colonnes dans le CSV\n",
        "EMOTION_LABELS = [\n",
        "    'Disconnection',\n",
        "    'Doubt/Confusion',\n",
        "    'Fatigue',\n",
        "    'Pain',\n",
        "    'Disquietment',\n",
        "    'Annoyance',\n",
        "    'others',\n",
        "    'adhd_emotion'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(EMOTION_LABELS)\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "FREEZE_FEATURES = True # True pour ne fine-tuner que la dernière couche, False pour fine-tuner tout le modèle\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation du périphérique : {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(DATA_DIR)\n",
        "# print(CSV_FILE)\n",
        "# print(IMAGE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Charger le fichier CSV des annotations\n",
        "# df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# # Vérifie qu'une colonne 'image' ou 'filename' existe\n",
        "# assert 'image' in df.columns or 'filename' in df.columns, \"❌ Le fichier CSV ne contient pas de colonne 'image' ou 'filename'.\"\n",
        "\n",
        "# # Nom de la colonne contenant les noms d'image\n",
        "# image_column = 'image' if 'image' in df.columns else 'filename'\n",
        "\n",
        "# # Vérification de l'existence des fichiers image\n",
        "# missing_images = []\n",
        "# for img_name in df[image_column].unique():\n",
        "#     img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "#     if not os.path.isfile(img_path):\n",
        "#         missing_images.append(img_name)\n",
        "\n",
        "# # Affichage du résultat\n",
        "# if missing_images:\n",
        "#     print(f\"❌ {len(missing_images)} image(s) manquante(s) sur {df[image_column].nunique()} :\")\n",
        "#     print(missing_images[:10])  # Affiche les 10 premières si la liste est longue\n",
        "# else:\n",
        "#     print(\"✅ Toutes les images référencées dans le CSV sont présentes dans le dossier images/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# II/ Création du Dataset PyTorch personnalisé (modifié)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7i1HiG6PTReS"
      },
      "outputs": [],
      "source": [
        "# --- 1. Création du Dataset PyTorch personnalisé (modifié) ---\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, emotion_cols, bbox_cols, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.emotion_cols = emotion_cols\n",
        "        self.bbox_cols = bbox_cols # Nouvelle ligne pour les colonnes de la bbox\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "\n",
        "        emotion_labels = row[self.emotion_cols].values.astype(float)\n",
        "        emotion_labels = torch.tensor(emotion_labels, dtype=torch.float32)\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        x1, y1, x2, y2 = row[self.bbox_cols].values.astype(int)\n",
        "        cropped_image = image.crop((x1, y1, x2, y2))\n",
        "        \n",
        "        if self.transform:\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        return cropped_image, emotion_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Chargement du CSV\n",
        "# df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# # Prendre une ligne aléatoire\n",
        "# sample_row = df.sample(1).iloc[0]\n",
        "# img_name = sample_row['image']  # colonne contenant le nom du fichier image\n",
        "# img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "\n",
        "# print(f\"Test de lecture de l'image : {img_path}\")\n",
        "\n",
        "# # Mesure du temps de lecture\n",
        "# start = time.time()\n",
        "# try:\n",
        "#     img = Image.open(img_path).convert('RGB')\n",
        "#     img.load()  # Force le chargement complet\n",
        "#     print(f\"✅ Image lue correctement. Taille = {img.size}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"❌ Erreur de lecture de l'image : {e}\")\n",
        "\n",
        "# end = time.time()\n",
        "# print(f\"⏱ Temps de lecture : {end - start:.4f} secondes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# III/ Transformations d'images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LkV1CWUbTUJB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\anaconda3\\envs\\dl_project_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Transformations d'images ---\n",
        "# Transformations pour l'entraînement (augmentation des données)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224), # Recadrage aléatoire et redimensionnement à 224x224\n",
        "        transforms.RandomHorizontalFlip(), # Retournement horizontal aléatoire\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Jitter de couleur\n",
        "        transforms.ToTensor(), # Convertir en Tensor PyTorch (met les pixels entre 0 et 1)\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalisation (moyenne et std ImageNet)\n",
        "    ]),\n",
        "    # Transformations pour la validation/test (juste redimensionnement et normalisation)\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256), # Redimensionner le plus petit côté à 256\n",
        "        transforms.CenterCrop(224), # Centre Crop à 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IV/ Chargement des données et division Train/Val "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-mNFgbICTrGD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement des données...\n",
            "Taille du dataset d'entraînement : 13661\n",
            "Taille du dataset de validation : 3416\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Chargement des données et division Train/Val ---\n",
        "print(\"Chargement des données...\")\n",
        "full_df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "# Division du dataset en ensembles d'entraînement et de validation\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df['adhd_emotion'] if True else None) # Stratify si possible\n",
        "\n",
        "train_dataset = EmotionDataset(df=train_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, bbox_cols=BODY_BBOX_COLS, transform=data_transforms['train'])\n",
        "val_dataset = EmotionDataset(df=val_df, img_dir=IMAGE_DIR, emotion_cols=EMOTION_LABELS, bbox_cols=BODY_BBOX_COLS, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "print(f\"Taille du dataset d'entraînement : {dataset_sizes['train']}\")\n",
        "print(f\"Taille du dataset de validation : {dataset_sizes['val']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_dataset[0]  # ou un autre index si ça bloque toujours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next(iter(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Test 0 : Vérifie si le DataLoader donne quelque chose ---\n",
        "# print(\"Test du premier batch dans val_loader...\")\n",
        "\n",
        "# try:\n",
        "#     sample_batch = next(iter(val_loader))\n",
        "#     images, labels = sample_batch\n",
        "#     print(f\"Batch chargé : {images.shape} - {labels.shape}\")\n",
        "# except Exception as e:\n",
        "#     print(\"Erreur pendant le chargement du batch :\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Test 1 : Vérifie si le DataLoader donne quelque chose ---\n",
        "# print(\"Test du premier batch dans train_loader...\")\n",
        "\n",
        "# try:\n",
        "#     sample_batch = next(iter(train_loader))\n",
        "#     images, labels = sample_batch\n",
        "#     print(f\"Batch chargé : {images.shape} - {labels.shape}\")\n",
        "# except Exception as e:\n",
        "#     print(\"Erreur pendant le chargement du batch :\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Active le DataLoader uniquement\n",
        "# print(\"⏳ Test de vitesse du train_loader...\")\n",
        "\n",
        "# n_batches_to_test = 20  # Tu peux ajuster selon la taille de ton dataset\n",
        "# total_load_time = 0.0\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# for i, (inputs, labels) in enumerate(train_loader):\n",
        "#     if i >= n_batches_to_test:\n",
        "#         break\n",
        "\n",
        "#     start = time.time()\n",
        "\n",
        "#     # Envoie au GPU (si utile pour tester l'impact)\n",
        "#     inputs = inputs.to(device)\n",
        "#     labels = labels.to(device)\n",
        "\n",
        "#     # Attendre que les données soient vraiment transférées\n",
        "#     torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
        "\n",
        "#     end = time.time()\n",
        "#     load_time = end - start\n",
        "#     total_load_time += load_time\n",
        "#     print(f\"🧱 Batch {i+1} - Temps de chargement + transfert : {load_time:.4f} sec\")\n",
        "\n",
        "# print(\"\\n📊 Moyenne sur\", n_batches_to_test, \"batches :\")\n",
        "# print(f\"⏱ Temps moyen par batch : {total_load_time / n_batches_to_test:.4f} sec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i in range (2):\n",
        "#     print(train_dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V/ Chargement du modèle ResNet18 pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement et modification du modèle ResNet18 pré-entraîné...\n",
            "Freeze des couches convolutionnelles...\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.bn1.bias\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.bn2.bias\n",
            "layer4.0.downsample.0.weight\n",
            "layer4.0.downsample.1.weight\n",
            "layer4.0.downsample.1.bias\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.bn1.bias\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "layer4.1.bn2.bias\n",
            "fc.weight\n",
            "fc.bias\n",
            "Définition de la dernière couche...\n",
            "Modèle prêt ! ✅\n"
          ]
        }
      ],
      "source": [
        "print(\"Chargement et modification du modèle ResNet18 pré-entraîné...\")\n",
        "\n",
        "# 1. Chargement du modèle ResNet18 pré-entraîné sur ImageNet\n",
        "model_ft = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"Freeze des couches convolutionnelles...\")\n",
        "\n",
        "# 2. On gèle les couches de feature extraction\n",
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#fine_tune_at = len(list(model_ft.children())) - 10\n",
        "for name, param in list(model_ft.named_parameters())[-17:]:\n",
        "    print(name)\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"Définition de la dernière couche...\")\n",
        "\n",
        "# 3. Remplacement de la couche fully-connected finale\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "model_ft.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(512, NUM_CLASSES)\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Envoi du modèle sur le bon device (GPU ou CPU)\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "\n",
        "print(\"Modèle prêt ! ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(model_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [1, 8]                    --\n",
              "├─Conv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
              "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
              "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
              "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           (36,864)\n",
              "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
              "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
              "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
              "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           (36,864)\n",
              "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           (128)\n",
              "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           (36,864)\n",
              "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           (128)\n",
              "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          (73,728)\n",
              "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          (256)\n",
              "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          (147,456)\n",
              "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          (256)\n",
              "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          (8,448)\n",
              "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          (147,456)\n",
              "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          (256)\n",
              "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          (147,456)\n",
              "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          (256)\n",
              "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
              "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          (294,912)\n",
              "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          (512)\n",
              "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          (589,824)\n",
              "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          (512)\n",
              "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          (33,280)\n",
              "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          (589,824)\n",
              "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          (512)\n",
              "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          (589,824)\n",
              "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          (512)\n",
              "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
              "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
              "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
              "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
              "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
              "├─Sequential: 1-10                       [1, 8]                    --\n",
              "│    └─Linear: 2-9                       [1, 512]                  262,656\n",
              "│    └─BatchNorm1d: 2-10                 [1, 512]                  1,024\n",
              "│    └─ReLU: 2-11                        [1, 512]                  --\n",
              "│    └─Dropout: 2-12                     [1, 512]                  --\n",
              "│    └─Linear: 2-13                      [1, 8]                    4,104\n",
              "==========================================================================================\n",
              "Total params: 11,444,296\n",
              "Trainable params: 8,661,512\n",
              "Non-trainable params: 2,782,784\n",
              "Total mult-adds (Units.GIGABYTES): 1.81\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 39.75\n",
              "Params size (MB): 45.78\n",
              "Estimated Total Size (MB): 86.13\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print model summary\n",
        "#summary(model_ft, input_size=(8, 3, 224, 224))  # (batch_size, input_features)\n",
        "summary(model_ft, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VI/ Fonction de perte et Optimiseur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16,\n",
            " 35,\n",
            " 40,\n",
            " 126,\n",
            " 46,\n",
            " 61,\n",
            " 1,\n",
            " 8\n"
          ]
        }
      ],
      "source": [
        "# definition des poids pour chaques emotions test 2  : \n",
        "\n",
        "Disconnection_w =  round(17077 / 1043)\n",
        "Doubt_Confusion_w = round(17077 / 489)\n",
        "Fatigue_w = round(17077 / 429)\n",
        "Pain_w = round(17077 / 135)\n",
        "Disquietment_w = round(17077 / 368)\n",
        "Annoyance_w = round(17077 / 278)\n",
        "Others_w = round(17077 / 16163)\n",
        "Adhd_emotion_w = round(17077 / 2211)\n",
        "\n",
        "print(f'{Disconnection_w},\\n {Doubt_Confusion_w},\\n {Fatigue_w},\\n {Pain_w},\\n {Disquietment_w},\\n {Annoyance_w},\\n {Others_w},\\n {Adhd_emotion_w}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_weights = torch.tensor([\n",
        "    Disconnection_w,  # Disconnection\n",
        "    Doubt_Confusion_w,  # Doubt/Confusion\n",
        "    Fatigue_w,  # Fatigue\n",
        "    Pain_w,  # Pain\n",
        "    Disquietment_w,  # Disquietment\n",
        "    Annoyance_w,  # Annoyance\n",
        "    Others_w,  # others\n",
        "    Adhd_emotion_w   # adhd_emotion\n",
        "], dtype=torch.float32).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(emotion_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.log2(emotion_weights*2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAV_C7MYUH2A"
      },
      "outputs": [],
      "source": [
        "# --- 5. Fonction de perte et Optimiseur ---\n",
        "# Pour la classification multi-label, Binary Cross-Entropy with Logits Loss est la norme.\n",
        "# Elle combine une couche Sigmoid et la Binary Cross-Entropy.\n",
        "# Perte pour la classification multi-label des émotions\n",
        "\n",
        "criterion_emotions = nn.BCEWithLogitsLoss(pos_weight=torch.log2(emotion_weights*2))\n",
        "\n",
        "# Seulement les paramètres qui nécessitent des gradients seront optimisés\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Ordonnanceur de taux d'apprentissage (réduit le LR après un certain nombre d'époques)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VII/ Fonction d'entraînement et d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Fonction d'entraînement et d'évaluation ---\n",
        "\n",
        "def train_model(model, criterion_emotions, optimizer, train_loader, val_loader, device, scheduler=None, epochs=NUM_EPOCHS):\n",
        "    \n",
        "    since = time.time()\n",
        "    \n",
        "    history = {'train_loss': [], 'val_loss': [],'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print('-' * 30)\n",
        "\n",
        "        ### -------- TRAIN --------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion_emotions(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Accuracy multi-label (threshold 0.5)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            running_corrects += (preds == labels).float().sum().item()\n",
        "            total_samples += labels.numel()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / total_samples\n",
        "\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "\n",
        "        ### -------- VAL --------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_emotions(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "                val_corrects += (preds == labels).float().sum().item()\n",
        "                val_total += labels.numel()\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_corrects / val_total\n",
        "\n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "        history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "        # Liste pour stocker toutes les prédictions et les labels réels sur l'ensemble de validation\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_emotions(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                preds = (probs > 0.5).float()\n",
        "\n",
        "                val_corrects += (preds == labels).float().sum().item()\n",
        "                val_total += labels.numel()\n",
        "\n",
        "                all_preds.append(preds.cpu())\n",
        "                all_labels.append(labels.cpu())\n",
        "\n",
        "        # Concaténation de tous les batches\n",
        "        all_preds = torch.cat(all_preds).numpy()\n",
        "        all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "        # Affichage du rapport classification multilabel\n",
        "        print(\"\\n📊 Rapport de classification (Validation):\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=EMOTION_LABELS, zero_division=0))\n",
        "        \n",
        "        print(f\"Train Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
        "        print(f\"Val   Loss: {val_epoch_loss:.4f} | Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"\\n🕒 Entraînement terminé en {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VIII/ Lancement de l'entraînement "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Début de l'entraînement...\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.08      0.65      0.15       200\n",
            "Doubt/Confusion       0.04      0.57      0.08        98\n",
            "        Fatigue       0.04      0.77      0.07        88\n",
            "           Pain       0.01      0.58      0.02        31\n",
            "   Disquietment       0.03      0.36      0.06        75\n",
            "      Annoyance       0.02      0.60      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.17      0.62      0.27       442\n",
            "\n",
            "      micro avg       0.29      0.91      0.44      4216\n",
            "      macro avg       0.17      0.64      0.21      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.42      0.96      0.53      4216\n",
            "\n",
            "Train Loss: 1.3402 | Acc: 0.5589\n",
            "Val   Loss: 1.2257 | Acc: 0.6445\n",
            "\n",
            "Epoch 2/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.64      0.16       200\n",
            "Doubt/Confusion       0.05      0.53      0.09        98\n",
            "        Fatigue       0.05      0.73      0.09        88\n",
            "           Pain       0.02      0.48      0.03        31\n",
            "   Disquietment       0.04      0.61      0.07        75\n",
            "      Annoyance       0.02      0.74      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.17      0.71      0.28       442\n",
            "\n",
            "      micro avg       0.30      0.92      0.46      4216\n",
            "      macro avg       0.17      0.68      0.22      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.44      0.96      0.54      4216\n",
            "\n",
            "Train Loss: 1.2001 | Acc: 0.6135\n",
            "Val   Loss: 1.1976 | Acc: 0.6605\n",
            "\n",
            "Epoch 3/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.59      0.16       200\n",
            "Doubt/Confusion       0.05      0.45      0.09        98\n",
            "        Fatigue       0.04      0.66      0.08        88\n",
            "           Pain       0.02      0.61      0.04        31\n",
            "   Disquietment       0.04      0.45      0.07        75\n",
            "      Annoyance       0.02      0.57      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.65      0.29       442\n",
            "\n",
            "      micro avg       0.32      0.91      0.48      4216\n",
            "      macro avg       0.17      0.62      0.22      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.47      0.96      0.57      4216\n",
            "\n",
            "Train Loss: 1.1849 | Acc: 0.6266\n",
            "Val   Loss: 1.1946 | Acc: 0.6920\n",
            "\n",
            "Epoch 4/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.08      0.70      0.14       200\n",
            "Doubt/Confusion       0.04      0.59      0.08        98\n",
            "        Fatigue       0.05      0.61      0.09        88\n",
            "           Pain       0.02      0.39      0.03        31\n",
            "   Disquietment       0.04      0.63      0.07        75\n",
            "      Annoyance       0.02      0.58      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.17      0.67      0.27       442\n",
            "\n",
            "      micro avg       0.30      0.92      0.45      4216\n",
            "      macro avg       0.17      0.65      0.21      4216\n",
            "   weighted avg       0.75      0.92      0.78      4216\n",
            "    samples avg       0.46      0.96      0.55      4216\n",
            "\n",
            "Train Loss: 1.1879 | Acc: 0.6276\n",
            "Val   Loss: 1.2165 | Acc: 0.6551\n",
            "\n",
            "Epoch 5/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.08      0.65      0.14       200\n",
            "Doubt/Confusion       0.05      0.53      0.09        98\n",
            "        Fatigue       0.05      0.72      0.09        88\n",
            "           Pain       0.02      0.61      0.04        31\n",
            "   Disquietment       0.03      0.53      0.06        75\n",
            "      Annoyance       0.02      0.47      0.03        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.69      0.28       442\n",
            "\n",
            "      micro avg       0.30      0.92      0.45      4216\n",
            "      macro avg       0.17      0.65      0.21      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.47      0.96      0.56      4216\n",
            "\n",
            "Train Loss: 1.1766 | Acc: 0.6355\n",
            "Val   Loss: 1.2153 | Acc: 0.6563\n",
            "\n",
            "Epoch 6/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.61      0.15       200\n",
            "Doubt/Confusion       0.04      0.56      0.08        98\n",
            "        Fatigue       0.05      0.75      0.09        88\n",
            "           Pain       0.02      0.42      0.03        31\n",
            "   Disquietment       0.03      0.57      0.05        75\n",
            "      Annoyance       0.02      0.66      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.68      0.28       442\n",
            "\n",
            "      micro avg       0.29      0.92      0.45      4216\n",
            "      macro avg       0.17      0.66      0.21      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.44      0.96      0.54      4216\n",
            "\n",
            "Train Loss: 1.1769 | Acc: 0.6324\n",
            "Val   Loss: 1.2176 | Acc: 0.6489\n",
            "\n",
            "Epoch 7/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.10      0.54      0.16       200\n",
            "Doubt/Confusion       0.04      0.63      0.08        98\n",
            "        Fatigue       0.05      0.77      0.10        88\n",
            "           Pain       0.02      0.58      0.04        31\n",
            "   Disquietment       0.04      0.57      0.07        75\n",
            "      Annoyance       0.03      0.43      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.20      0.59      0.30       442\n",
            "\n",
            "      micro avg       0.33      0.90      0.49      4216\n",
            "      macro avg       0.18      0.64      0.22      4216\n",
            "   weighted avg       0.75      0.90      0.79      4216\n",
            "    samples avg       0.49      0.95      0.59      4216\n",
            "\n",
            "Train Loss: 1.1657 | Acc: 0.6318\n",
            "Val   Loss: 1.1693 | Acc: 0.7047\n",
            "\n",
            "Epoch 8/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.10      0.66      0.17       200\n",
            "Doubt/Confusion       0.05      0.61      0.09        98\n",
            "        Fatigue       0.05      0.72      0.10        88\n",
            "           Pain       0.02      0.45      0.03        31\n",
            "   Disquietment       0.04      0.59      0.07        75\n",
            "      Annoyance       0.02      0.43      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.66      0.29       442\n",
            "\n",
            "      micro avg       0.33      0.91      0.49      4216\n",
            "      macro avg       0.18      0.64      0.22      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.51      0.96      0.60      4216\n",
            "\n",
            "Train Loss: 1.1327 | Acc: 0.6521\n",
            "Val   Loss: 1.1516 | Acc: 0.7009\n",
            "\n",
            "Epoch 9/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.10      0.67      0.17       200\n",
            "Doubt/Confusion       0.05      0.62      0.09        98\n",
            "        Fatigue       0.05      0.73      0.10        88\n",
            "           Pain       0.02      0.52      0.04        31\n",
            "   Disquietment       0.04      0.59      0.07        75\n",
            "      Annoyance       0.02      0.45      0.05        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.67      0.28       442\n",
            "\n",
            "      micro avg       0.32      0.92      0.48      4216\n",
            "      macro avg       0.18      0.66      0.22      4216\n",
            "   weighted avg       0.75      0.92      0.79      4216\n",
            "    samples avg       0.51      0.96      0.59      4216\n",
            "\n",
            "Train Loss: 1.1126 | Acc: 0.6581\n",
            "Val   Loss: 1.1479 | Acc: 0.6913\n",
            "\n",
            "Epoch 10/10\n",
            "------------------------------\n",
            "\n",
            "📊 Rapport de classification (Validation):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "  Disconnection       0.09      0.68      0.16       200\n",
            "Doubt/Confusion       0.05      0.61      0.09        98\n",
            "        Fatigue       0.05      0.62      0.10        88\n",
            "           Pain       0.02      0.45      0.04        31\n",
            "   Disquietment       0.04      0.57      0.07        75\n",
            "      Annoyance       0.02      0.38      0.04        53\n",
            "         others       0.95      1.00      0.97      3229\n",
            "   adhd_emotion       0.18      0.66      0.29       442\n",
            "\n",
            "      micro avg       0.34      0.91      0.49      4216\n",
            "      macro avg       0.18      0.62      0.22      4216\n",
            "   weighted avg       0.75      0.91      0.79      4216\n",
            "    samples avg       0.52      0.96      0.61      4216\n",
            "\n",
            "Train Loss: 1.1076 | Acc: 0.6593\n",
            "Val   Loss: 1.1536 | Acc: 0.7085\n",
            "\n",
            "🕒 Entraînement terminé en 39m 34s\n"
          ]
        }
      ],
      "source": [
        "# --- 7. Lancement de l'entraînement ---\n",
        "print(\"\\nDébut de l'entraînement...\\n\")\n",
        "history = train_model(model=model_ft,\n",
        "                        criterion_emotions=criterion_emotions,\n",
        "                        optimizer=optimizer_ft,\n",
        "                        train_loader=train_loader,\n",
        "                        val_loader=val_loader,\n",
        "                        device=DEVICE,\n",
        "                        scheduler=exp_lr_scheduler,\n",
        "                        epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IX/ Enregistrement model + export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chemin de sauvegarde\n",
        "onnx_export_path = \"emotic_model.onnx\"\n",
        "\n",
        "# Dummy input — doit correspondre à la taille attendue par ton modèle\n",
        "dummy_input = torch.randn(1, 3, 224, 224, device=DEVICE)\n",
        "\n",
        "# Export ONNX\n",
        "torch.onnx.export(\n",
        "    model_ft,                  # Le modèle entraîné\n",
        "    dummy_input,               # Un exemple d'input\n",
        "    onnx_export_path,          # Chemin de sortie\n",
        "    input_names=['input'],     # Nom de l'input\n",
        "    output_names=['output'],   # Nom de la sortie\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},\n",
        "        'output': {0: 'batch_size'}\n",
        "    },\n",
        "    opset_version=11,          # Version de l'opset ONNX (11 est sûr pour la compatibilité)\n",
        "    do_constant_folding=True   # Optimisation pour les constantes\n",
        ")\n",
        "\n",
        "print(f\"✅ Modèle exporté au format ONNX : {onnx_export_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zOnRjwFUhgo"
      },
      "outputs": [],
      "source": [
        "# # --- 8. Sauvegarde du modèle entraîné ---\n",
        "# # Créez un dossier pour les modèles si n'existe pas\n",
        "# os.makedirs('saved_models', exist_ok=True)\n",
        "# model_save_path = os.path.join('saved_models', 'resnet18_emotion_dav_multi_person.pth')\n",
        "# torch.save(model_ft.state_dict(), model_save_path)\n",
        "# print(f\"Modèle sauvegardé à : {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AR5Dl7-GCrI"
      },
      "outputs": [],
      "source": [
        "# --- 9. (Optionnel) Évaluation finale sur l'ensemble de validation ---\n",
        "# Charger le modèle pour l'évaluation\n",
        "# model_ft.eval() # Mettre en mode évaluation\n",
        "# ... Vous pouvez réutiliser le code d'évaluation de la fonction train_model ici si vous voulez une évaluation finale séparée.\n",
        "\n",
        "print(\"\\n--- Entraînement terminé ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import onnxruntime as ort\n",
        "# import numpy as np\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "\n",
        "# # Charger le modèle ONNX\n",
        "# session = ort.InferenceSession(\"emotic_model.onnx\")\n",
        "\n",
        "# # Charger et prétraiter une image\n",
        "# img_path = \"chemin/vers/image.jpg\"\n",
        "# image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406], \n",
        "#                          [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# input_tensor = transform(image).unsqueeze(0).numpy()  # (1, 3, 224, 224)\n",
        "\n",
        "# # Faire une prédiction\n",
        "# outputs = session.run(['output'], {'input': input_tensor})\n",
        "# preds = outputs[0]\n",
        "\n",
        "# # Résultat\n",
        "# print(\"Prédiction brute :\", preds)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl_project_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
